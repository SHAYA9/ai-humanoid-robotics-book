{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/ai-humanoid-robotics-book/intro","label":"üöÄ Introduction","docId":"intro","unlisted":false},{"type":"category","label":"üìö AI Foundations","items":[{"type":"link","href":"/ai-humanoid-robotics-book/ai-foundations/overview","label":"overview","docId":"ai-foundations/overview","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/ai-foundations/machine-learning","label":"machine-learning","docId":"ai-foundations/machine-learning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/ai-foundations/deep-learning","label":"deep-learning","docId":"ai-foundations/deep-learning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/ai-foundations/computer-vision","label":"computer-vision","docId":"ai-foundations/computer-vision","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/ai-foundations/nlp","label":"nlp","docId":"ai-foundations/nlp","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/ai-foundations/reinforcement-learning","label":"reinforcement-learning","docId":"ai-foundations/reinforcement-learning","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/ai-foundations"},{"type":"category","label":"ü§ñ Humanoid Robotics","items":[{"type":"link","href":"/ai-humanoid-robotics-book/humanoid-robotics/design-principles","label":"design-principles","docId":"humanoid-robotics/design-principles","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/humanoid-robotics/kinematics-dynamics","label":"kinematics-dynamics","docId":"humanoid-robotics/kinematics-dynamics","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/humanoid-robotics/motion-planning","label":"motion-planning","docId":"humanoid-robotics/motion-planning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/humanoid-robotics/balance-control","label":"balance-control","docId":"humanoid-robotics/balance-control","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/humanoid-robotics/walking-gait","label":"walking-gait","docId":"humanoid-robotics/walking-gait","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/humanoid-robotics"},{"type":"category","label":"üîå Sensor Integration","items":[{"type":"link","href":"/ai-humanoid-robotics-book/sensor-integration/overview","label":"overview","docId":"sensor-integration/overview","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/sensor-integration/lidar-depth","label":"lidar-depth","docId":"sensor-integration/lidar-depth","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/sensor-integration/imu-sensors","label":"imu-sensors","docId":"sensor-integration/imu-sensors","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/sensor-integration/computer-vision","label":"computer-vision","docId":"sensor-integration/computer-vision","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/sensor-integration/touch-sensors","label":"touch-sensors","docId":"sensor-integration/touch-sensors","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/sensor-integration/sensor-fusion","label":"sensor-fusion","docId":"sensor-integration/sensor-fusion","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/sensor-integration"},{"type":"category","label":"üíª Programming","items":[{"type":"link","href":"/ai-humanoid-robotics-book/programming/ros-2","label":"ros-2","docId":"programming/ros-2","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/programming/python-robotics","label":"python-robotics","docId":"programming/python-robotics","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/programming/cpp-robotics","label":"cpp-robotics","docId":"programming/cpp-robotics","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/programming/real-time-systems","label":"real-time-systems","docId":"programming/real-time-systems","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/programming/simulation","label":"simulation","docId":"programming/simulation","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/programming"},{"type":"category","label":"üöÄ Advanced Topics","items":[{"type":"link","href":"/ai-humanoid-robotics-book/advanced-topics/autonomous-navigation","label":"autonomous-navigation","docId":"advanced-topics/autonomous-navigation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/advanced-topics/edge-ai","label":"edge-ai","docId":"advanced-topics/edge-ai","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/advanced-topics/ai-ethics","label":"ai-ethics","docId":"advanced-topics/ai-ethics","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/advanced-topics/hardware-acceleration","label":"hardware-acceleration","docId":"advanced-topics/hardware-acceleration","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/advanced-topics/cloud-robotics","label":"cloud-robotics","docId":"advanced-topics/cloud-robotics","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/advanced-topics"},{"type":"category","label":"üèÜ Projects","items":[{"type":"link","href":"/ai-humanoid-robotics-book/projects/walking-robot","label":"walking-robot","docId":"projects/walking-robot","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/projects/object-recognition","label":"object-recognition","docId":"projects/object-recognition","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/projects/autonomous-navigation","label":"autonomous-navigation","docId":"projects/autonomous-navigation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/projects/human-robot-interaction","label":"human-robot-interaction","docId":"projects/human-robot-interaction","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/projects/capstone-project","label":"capstone-project","docId":"projects/capstone-project","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/projects"},{"type":"category","label":"üìù Resources","items":[{"type":"link","href":"/ai-humanoid-robotics-book/resources/cheat-sheets","label":"cheat-sheets","docId":"resources/cheat-sheets","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/resources/tools-software","label":"tools-software","docId":"resources/tools-software","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/resources/books-references","label":"books-references","docId":"resources/books-references","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/resources/research-papers","label":"research-papers","docId":"resources/research-papers","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/resources/contributing","label":"contributing","docId":"resources/contributing","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/ai-humanoid-robotics-book/category/resources"},{"type":"link","href":"/ai-humanoid-robotics-book/faq","label":"‚ùì FAQ","docId":"faq","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/glossary","label":"üìñ Glossary","docId":"glossary","unlisted":false}]},"docs":{"advanced-topics/ai-ethics":{"id":"advanced-topics/ai-ethics","title":"ai-ethics","description":"ÔøΩÔøΩ#\u0000 \u0000A\u0000I\u0000 \u0000E\u0000t\u0000h\u0000i\u0000c\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"advanced-topics/autonomous-navigation":{"id":"advanced-topics/autonomous-navigation","title":"autonomous-navigation","description":"ÔøΩÔøΩ#\u0000 \u0000A\u0000u\u0000t\u0000o\u0000n\u0000o\u0000m\u0000o\u0000u\u0000s\u0000 \u0000N\u0000a\u0000v\u0000i\u0000g\u0000a\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"advanced-topics/cloud-robotics":{"id":"advanced-topics/cloud-robotics","title":"cloud-robotics","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000l\u0000o\u0000u\u0000d\u0000 \u0000R\u0000o\u0000b\u0000o\u0000t\u0000i\u0000c\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"advanced-topics/edge-ai":{"id":"advanced-topics/edge-ai","title":"edge-ai","description":"ÔøΩÔøΩ#\u0000 \u0000E\u0000d\u0000g\u0000e\u0000 \u0000A\u0000I\u0000\r\u0000","sidebar":"tutorialSidebar"},"advanced-topics/hardware-acceleration":{"id":"advanced-topics/hardware-acceleration","title":"hardware-acceleration","description":"ÔøΩÔøΩ#\u0000 \u0000H\u0000a\u0000r\u0000d\u0000w\u0000a\u0000r\u0000e\u0000 \u0000A\u0000c\u0000c\u0000e\u0000l\u0000e\u0000r\u0000a\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"ai-foundations/computer-vision":{"id":"ai-foundations/computer-vision","title":"computer-vision","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000o\u0000m\u0000p\u0000u\u0000t\u0000e\u0000r\u0000 \u0000V\u0000i\u0000s\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"ai-foundations/deep-learning":{"id":"ai-foundations/deep-learning","title":"deep-learning","description":"ÔøΩÔøΩ#\u0000 \u0000D\u0000e\u0000e\u0000p\u0000 \u0000L\u0000e\u0000a\u0000r\u0000n\u0000i\u0000n\u0000g\u0000\r\u0000","sidebar":"tutorialSidebar"},"ai-foundations/machine-learning":{"id":"ai-foundations/machine-learning","title":"machine-learning","description":"ÔøΩÔøΩ#\u0000 \u0000M\u0000a\u0000c\u0000h\u0000i\u0000n\u0000e\u0000 \u0000L\u0000e\u0000a\u0000r\u0000n\u0000i\u0000n\u0000g\u0000\r\u0000","sidebar":"tutorialSidebar"},"ai-foundations/nlp":{"id":"ai-foundations/nlp","title":"nlp","description":"ÔøΩÔøΩ#\u0000 \u0000N\u0000a\u0000t\u0000u\u0000r\u0000a\u0000l\u0000 \u0000L\u0000a\u0000n\u0000g\u0000u\u0000a\u0000g\u0000e\u0000 \u0000P\u0000r\u0000o\u0000c\u0000e\u0000s\u0000s\u0000i\u0000n\u0000g\u0000\r\u0000","sidebar":"tutorialSidebar"},"ai-foundations/overview":{"id":"ai-foundations/overview","title":"overview","description":"ÔøΩÔøΩ#\u0000 \u0000A\u0000I\u0000 \u0000F\u0000o\u0000u\u0000n\u0000d\u0000a\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000O\u0000v\u0000e\u0000r\u0000v\u0000i\u0000e\u0000w\u0000\r\u0000","sidebar":"tutorialSidebar"},"ai-foundations/reinforcement-learning":{"id":"ai-foundations/reinforcement-learning","title":"reinforcement-learning","description":"ÔøΩÔøΩ#\u0000 \u0000R\u0000e\u0000i\u0000n\u0000f\u0000o\u0000r\u0000c\u0000e\u0000m\u0000e\u0000n\u0000t\u0000 \u0000L\u0000e\u0000a\u0000r\u0000n\u0000i\u0000n\u0000g\u0000\r\u0000","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","title":"7.4 Domain Randomization","description":"One of the most powerful techniques for training robust models with synthetic data is Domain Randomization. The goal is to expose the model to a wide variety of scenarios during training so that it can generalize well to the real world, which is inherently unpredictable."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","title":"7.6 GPU Requirements","description":"The NVIDIA Isaac platform is incredibly powerful, but that power comes with specific hardware requirements. Unlike other ROS 2 tools that are primarily CPU-bound, Isaac Sim and Isaac ROS are fundamentally GPU-bound. The performance of your simulation and AI workloads will be directly tied to the capabilities of your NVIDIA graphics card."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","title":"7.1 Introduction to the NVIDIA Isaac Platform","description":"While Gazebo provides a powerful, open-source foundation for robotics simulation, NVIDIA Isaac is a comprehensive, enterprise-grade platform designed to accelerate the development and deployment of AI-powered robots. It's an ecosystem of tools that leverages NVIDIA's expertise in GPU technology to tackle the most demanding tasks in robotics, from photorealistic simulation to hardware-accelerated AI perception."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","title":"7.2 The Isaac Sim Pipeline","description":"Understanding the workflow and core components of Isaac Sim is key to leveraging its power. The pipeline is built on the NVIDIA Omniverse platform, which uses a unique set of technologies for describing, simulating, and rendering complex 3D worlds."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","title":"7.7 Labs and Benchmarks","description":"These labs will provide hands-on experience with the core components of the NVIDIA Isaac ecosystem. Please ensure your system meets the GPU requirements outlined in the previous section before proceeding."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","title":"7.5 Nav2 Path Planning Solutions","description":"The ultimate goal of simulation is to develop and test autonomous behaviors that can be deployed on a physical robot. The standard tool for robot navigation in the ROS 2 ecosystem is the Nav2 stack. A key advantage of the Isaac Sim platform is its seamless integration with Nav2, enabling a true sim-to-real workflow."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","title":"7.5 Reinforcement Learning in Isaac Sim","description":"Isaac Sim is purpose-built for training reinforcement learning (RL) agents. Its ability to run thousands of parallel simulations on a single GPU makes it possible to collect massive amounts of training data in a short amount of time."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","title":"7.6 Sim2Real: From Simulation to the Real World","description":"The ultimate goal of training in simulation is to deploy the learned policy on a physical robot. This process is known as Sim2Real transfer. A successful Sim2Real transfer is the holy grail of robotics simulation, and Isaac Sim is designed to make this process as smooth as possible."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","title":"7.3 Synthetic Data Generation (SDG)","description":"One of the most compelling reasons to use a photorealistic simulator like Isaac Sim is for Synthetic Data Generation (SDG). Training modern perception models, especially for object detection and segmentation, requires enormous, hand-labeled datasets. SDG automates this process, allowing you to generate perfectly labeled data at a massive scale."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","title":"7.4 VSLAM using Isaac ROS","description":"While Isaac Sim provides the world, Isaac ROS provides the robot's brain. Isaac ROS is a collection of high-performance ROS 2 packages, or Gems, that are hardware-accelerated to run on NVIDIA GPUs and Jetson devices. These Gems provide optimized implementations of common and computationally expensive robotics algorithms."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","title":"7.1 The Importance of Simulation","description":"In robotics, the cost of a mistake can be high. A software bug that might crash a web server could cause a physical robot to collide with an obstacle, damage itself, or pose a safety risk. This is why simulation is one of the most critical tools in the modern robotics development lifecycle."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","title":"7.5 Labs: From URDF to a Simulated Robot","description":"These labs will guide you through the process of creating a robot model, adding dynamics to it, and controlling it in the Gazebo simulator."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","title":"7.2 Physics Simulation Concepts","description":"To create a valuable Digital Twin, the simulation must be more than just a pretty picture; it must be physically plausible. Gazebo uses high-performance physics engines (like ODE or Bullet) to compute the effects of forces and collisions on your robot model, allowing you to test its dynamics in a realistic way."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","title":"7.3 Sensor Simulation","description":"One of the most powerful features of Gazebo is its ability to simulate a wide range of robotic sensors. The data generated by these virtual sensors is published to ROS 2 topics, just like data from real hardware. This allows you to test your entire perception and control stack in the simulation without needing a physical robot."},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","title":"7.4 Unity Visualization Plan","description":"While Gazebo is a powerful tool for physics simulation, its built-in rendering engine is optimized for speed, not photorealism. For applications that require high-fidelity graphics‚Äîsuch as generating synthetic data for AI, creating marketing materials, or developing user interfaces‚Äîit's common to pair Gazebo with a modern game engine like Unity."},"faq":{"id":"faq","title":"faq","description":"ÔøΩÔøΩ#\u0000 \u0000F\u0000A\u0000Q\u0000\r\u0000","sidebar":"tutorialSidebar"},"glossary":{"id":"glossary","title":"glossary","description":"ÔøΩÔøΩ#\u0000 \u0000G\u0000l\u0000o\u0000s\u0000s\u0000a\u0000r\u0000y\u0000\r\u0000","sidebar":"tutorialSidebar"},"humanoid-robotics/balance-control":{"id":"humanoid-robotics/balance-control","title":"balance-control","description":"ÔøΩÔøΩ#\u0000 \u0000B\u0000a\u0000l\u0000a\u0000n\u0000c\u0000e\u0000 \u0000C\u0000o\u0000n\u0000t\u0000r\u0000o\u0000l\u0000\r\u0000","sidebar":"tutorialSidebar"},"humanoid-robotics/design-principles":{"id":"humanoid-robotics/design-principles","title":"design-principles","description":"ÔøΩÔøΩ#\u0000 \u0000H\u0000u\u0000m\u0000a\u0000n\u0000o\u0000i\u0000d\u0000 \u0000D\u0000e\u0000s\u0000i\u0000g\u0000n\u0000 \u0000P\u0000r\u0000i\u0000n\u0000c\u0000i\u0000p\u0000l\u0000e\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"humanoid-robotics/kinematics-dynamics":{"id":"humanoid-robotics/kinematics-dynamics","title":"kinematics-dynamics","description":"ÔøΩÔøΩ#\u0000 \u0000K\u0000i\u0000n\u0000e\u0000m\u0000a\u0000t\u0000i\u0000c\u0000s\u0000 \u0000a\u0000n\u0000d\u0000 \u0000D\u0000y\u0000n\u0000a\u0000m\u0000i\u0000c\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"humanoid-robotics/motion-planning":{"id":"humanoid-robotics/motion-planning","title":"motion-planning","description":"ÔøΩÔøΩ#\u0000 \u0000M\u0000o\u0000t\u0000i\u0000o\u0000n\u0000 \u0000P\u0000l\u0000a\u0000n\u0000n\u0000i\u0000n\u0000g\u0000\r\u0000","sidebar":"tutorialSidebar"},"humanoid-robotics/walking-gait":{"id":"humanoid-robotics/walking-gait","title":"walking-gait","description":"ÔøΩÔøΩ#\u0000 \u0000W\u0000a\u0000l\u0000k\u0000i\u0000n\u0000g\u0000 \u0000G\u0000a\u0000i\u0000t\u0000\r\u0000","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"intro","description":"ÔøΩÔøΩ#\u0000 \u0000I\u0000n\u0000t\u0000r\u0000o\u0000d\u0000u\u0000c\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"introduction/how-to-use-this-book":{"id":"introduction/how-to-use-this-book","title":"How to Use This Book","description":""},"introduction/safety-primer":{"id":"introduction/safety-primer","title":"Safety First: A Robotics Primer","description":""},"introduction/welcome":{"id":"introduction/welcome","title":"Welcome to AI-Native Robotics","description":""},"labs-and-exercises/lab-01-isaac-sim-hello-world":{"id":"labs-and-exercises/lab-01-isaac-sim-hello-world","title":"Lab 1: Isaac Sim - Hello World","description":"Goal: Create a simple \"Hello World\" scene in Isaac Sim to get familiar with the interface and basic concepts."},"labs-and-exercises/lab-02-controlling-a-robot":{"id":"labs-and-exercises/lab-02-controlling-a-robot","title":"Lab 2: Controlling a Robot","description":"Goal: Add a Franka Emika Panda robot to the scene and control its joints."},"labs-and-exercises/lab-03-reinforcement-learning-reach-task":{"id":"labs-and-exercises/lab-03-reinforcement-learning-reach-task","title":"Lab 3: Reinforcement Learning - Reach Task","description":"Goal: Train a Franka robot to reach a target using reinforcement learning."},"labs-and-exercises/overview":{"id":"labs-and-exercises/overview","title":"Module 5: Labs and Exercises","description":"Welcome to the hands-on portion of this book! The following labs and exercises are designed to solidify your understanding of the concepts we've discussed. Each lab will provide you with a practical problem to solve, leveraging the tools and techniques covered in the previous modules."},"labs-and-exercises/vla-capstone-project/cognitive-planning":{"id":"labs-and-exercises/vla-capstone-project/cognitive-planning","title":"8.3 Cognitive Planning: Natural Language to Actions","description":"The most innovative part of our VLA architecture is the use of a Large Language Model (LLM) as a high-level task planner. The LLM's vast knowledge of language and reasoning allows it to bridge the ambiguous, flexible world of human commands with the structured, precise world of robot operations."},"labs-and-exercises/vla-capstone-project/introduction":{"id":"labs-and-exercises/vla-capstone-project/introduction","title":"8.1 The VLA Capstone Project","description":"Welcome to the final capstone project. This module integrates the key concepts from the entire book‚Äîrobotics foundations, hardware, AI control, and simulation‚Äîinto a single, ambitious project: building a Vision-Language-Action (VLA) system."},"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation":{"id":"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","title":"8.4 Object Detection and Manipulation","description":"For our robot to act on the world, it needs two key capabilities: perception to understand its environment and manipulation to interact with it. In our VLA system, these are handled by dedicated ROS 2 nodes that provide their functionality as services to the main planner."},"labs-and-exercises/vla-capstone-project/pipeline":{"id":"labs-and-exercises/vla-capstone-project/pipeline","title":"8.2 The Whisper ‚Üí LLM ‚Üí ROS 2 Pipeline","description":"The core of our VLA system is the data flow that converts human speech into a series of executable robot commands. This pipeline consists of three main stages, each handled by a dedicated ROS 2 node."},"learning-path":{"id":"learning-path","title":"learning-path","description":"ÔøΩÔøΩ#\u0000 \u0000L\u0000e\u0000a\u0000r\u0000n\u0000i\u0000n\u0000g\u0000 \u0000P\u0000a\u0000t\u0000h\u0000\r\u0000"},"programming/cpp-robotics":{"id":"programming/cpp-robotics","title":"cpp-robotics","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000+\u0000+\u0000 \u0000R\u0000o\u0000b\u0000o\u0000t\u0000i\u0000c\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"programming/python-robotics":{"id":"programming/python-robotics","title":"python-robotics","description":"ÔøΩÔøΩ#\u0000 \u0000P\u0000y\u0000t\u0000h\u0000o\u0000n\u0000 \u0000R\u0000o\u0000b\u0000o\u0000t\u0000i\u0000c\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"programming/real-time-systems":{"id":"programming/real-time-systems","title":"real-time-systems","description":"ÔøΩÔøΩ#\u0000 \u0000R\u0000e\u0000a\u0000l\u0000-\u0000t\u0000i\u0000m\u0000e\u0000 \u0000S\u0000y\u0000s\u0000t\u0000e\u0000m\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"programming/ros-2":{"id":"programming/ros-2","title":"ros-2","description":"ÔøΩÔøΩ#\u0000 \u0000R\u0000O\u0000S\u0000 \u00002\u0000\r\u0000","sidebar":"tutorialSidebar"},"programming/simulation":{"id":"programming/simulation","title":"simulation","description":"ÔøΩÔøΩ#\u0000 \u0000S\u0000i\u0000m\u0000u\u0000l\u0000a\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"projects/autonomous-navigation":{"id":"projects/autonomous-navigation","title":"autonomous-navigation","description":"ÔøΩÔøΩ#\u0000 \u0000A\u0000u\u0000t\u0000o\u0000n\u0000o\u0000m\u0000o\u0000u\u0000s\u0000 \u0000N\u0000a\u0000v\u0000i\u0000g\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000P\u0000r\u0000o\u0000j\u0000e\u0000c\u0000t\u0000\r\u0000","sidebar":"tutorialSidebar"},"projects/capstone-project":{"id":"projects/capstone-project","title":"capstone-project","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000a\u0000p\u0000s\u0000t\u0000o\u0000n\u0000e\u0000 \u0000P\u0000r\u0000o\u0000j\u0000e\u0000c\u0000t\u0000\r\u0000","sidebar":"tutorialSidebar"},"projects/human-robot-interaction":{"id":"projects/human-robot-interaction","title":"human-robot-interaction","description":"ÔøΩÔøΩ#\u0000 \u0000H\u0000u\u0000m\u0000a\u0000n\u0000-\u0000R\u0000o\u0000b\u0000o\u0000t\u0000 \u0000I\u0000n\u0000t\u0000e\u0000r\u0000a\u0000c\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"projects/object-recognition":{"id":"projects/object-recognition","title":"object-recognition","description":"ÔøΩÔøΩ#\u0000 \u0000O\u0000b\u0000j\u0000e\u0000c\u0000t\u0000 \u0000R\u0000e\u0000c\u0000o\u0000g\u0000n\u0000i\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"projects/walking-robot":{"id":"projects/walking-robot","title":"walking-robot","description":"ÔøΩÔøΩ#\u0000 \u0000W\u0000a\u0000l\u0000k\u0000i\u0000n\u0000g\u0000 \u0000R\u0000o\u0000b\u0000o\u0000t\u0000 \u0000P\u0000r\u0000o\u0000j\u0000e\u0000c\u0000t\u0000\r\u0000","sidebar":"tutorialSidebar"},"resources/books-references":{"id":"resources/books-references","title":"books-references","description":"ÔøΩÔøΩ#\u0000 \u0000B\u0000o\u0000o\u0000k\u0000s\u0000 \u0000a\u0000n\u0000d\u0000 \u0000R\u0000e\u0000f\u0000e\u0000r\u0000e\u0000n\u0000c\u0000e\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"resources/cheat-sheets":{"id":"resources/cheat-sheets","title":"cheat-sheets","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000h\u0000e\u0000a\u0000t\u0000 \u0000S\u0000h\u0000e\u0000e\u0000t\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"resources/contributing":{"id":"resources/contributing","title":"contributing","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000o\u0000n\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000n\u0000g\u0000 \u0000G\u0000u\u0000i\u0000d\u0000e\u0000\r\u0000","sidebar":"tutorialSidebar"},"resources/research-papers":{"id":"resources/research-papers","title":"research-papers","description":"ÔøΩÔøΩ#\u0000 \u0000R\u0000e\u0000s\u0000e\u0000a\u0000r\u0000c\u0000h\u0000 \u0000P\u0000a\u0000p\u0000e\u0000r\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"resources/tools-software":{"id":"resources/tools-software","title":"tools-software","description":"ÔøΩÔøΩ#\u0000 \u0000T\u0000o\u0000o\u0000l\u0000s\u0000 \u0000a\u0000n\u0000d\u0000 \u0000S\u0000o\u0000f\u0000t\u0000w\u0000a\u0000r\u0000e\u0000\r\u0000","sidebar":"tutorialSidebar"},"robotics-foundations/intro-to-robotics/what-is-a-robot":{"id":"robotics-foundations/intro-to-robotics/what-is-a-robot","title":"What is a Robot?","description":""},"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package":{"id":"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","title":"7.6 Building a ROS 2 Package","description":"So far, we've focused on writing and running individual nodes. However, the real power of ROS 2 comes from organizing your code into reusable, shareable units called packages. A package is a directory containing your nodes, launch files, custom message definitions, and a set of files that describe its contents and dependencies."},"robotics-foundations/the-robot-operating-system-ros-2/core-concepts":{"id":"robotics-foundations/the-robot-operating-system-ros-2/core-concepts","title":"7.2 Core Concepts: Nodes, Topics, and Messages","description":"At the heart of ROS 2 is a communication graph where independent programs exchange information. Understanding the three fundamental components of this graph‚ÄîNodes, Topics, and Messages‚Äîis the key to mastering ROS 2."},"robotics-foundations/the-robot-operating-system-ros-2/labs":{"id":"robotics-foundations/the-robot-operating-system-ros-2/labs","title":"7.7 Labs","description":"These labs will guide you through the practical steps of creating, building, and running a multi-node ROS 2 application. Following these exercises will solidify the core concepts discussed in this chapter."},"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity":{"id":"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","title":"7.4 Managing Complexity: Launch Files and Parameters","description":"As your robotics application grows from two nodes to ten, twenty, or even a hundred, starting and configuring each one manually becomes impossible. This is where ROS 2's launch system comes in. Launch files are powerful scripts that allow you to start, configure, and connect a complex system of nodes with a single command."},"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication":{"id":"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","title":"7.3 Synchronous Communication: Services and Actions","description":"While topics are perfect for continuous data streams, sometimes you need a more direct, synchronous form of communication. For this, ROS 2 provides two powerful mechanisms: Services for quick request/response interactions, and Actions for long-running tasks that require feedback."},"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging":{"id":"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","title":"7.5 Visualization and Debugging","description":"A running ROS 2 system is a complex, distributed network of nodes passing messages. Without the right tools, understanding what's happening can be nearly impossible. Fortunately, ROS 2 comes with a powerful suite of command-line and graphical tools for introspection, debugging, and visualization."},"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2":{"id":"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","title":"7.1 What is ROS 2?","description":"Welcome to the Robot Operating System (ROS 2), the communication backbone of modern robotics. While not a traditional operating system like Windows or Linux, ROS 2 provides a powerful framework of libraries and tools to help you build complex robot applications. Think of it less as an OS and more as a robotic nervous system‚Äîa standardized way for different parts of your robot to communicate and work together."},"sensor-integration/computer-vision":{"id":"sensor-integration/computer-vision","title":"computer-vision","description":"ÔøΩÔøΩ#\u0000 \u0000C\u0000o\u0000m\u0000p\u0000u\u0000t\u0000e\u0000r\u0000 \u0000V\u0000i\u0000s\u0000i\u0000o\u0000n\u0000 \u0000S\u0000e\u0000n\u0000s\u0000o\u0000r\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"sensor-integration/imu-sensors":{"id":"sensor-integration/imu-sensors","title":"imu-sensors","description":"ÔøΩÔøΩ#\u0000 \u0000I\u0000M\u0000U\u0000 \u0000S\u0000e\u0000n\u0000s\u0000o\u0000r\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"sensor-integration/lidar-depth":{"id":"sensor-integration/lidar-depth","title":"lidar-depth","description":"ÔøΩÔøΩ#\u0000 \u0000L\u0000i\u0000D\u0000A\u0000R\u0000 \u0000a\u0000n\u0000d\u0000 \u0000D\u0000e\u0000p\u0000t\u0000h\u0000 \u0000S\u0000e\u0000n\u0000s\u0000o\u0000r\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"},"sensor-integration/overview":{"id":"sensor-integration/overview","title":"overview","description":"ÔøΩÔøΩ#\u0000 \u0000S\u0000e\u0000n\u0000s\u0000o\u0000r\u0000 \u0000I\u0000n\u0000t\u0000e\u0000g\u0000r\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000O\u0000v\u0000e\u0000r\u0000v\u0000i\u0000e\u0000w\u0000\r\u0000","sidebar":"tutorialSidebar"},"sensor-integration/sensor-fusion":{"id":"sensor-integration/sensor-fusion","title":"sensor-fusion","description":"ÔøΩÔøΩ#\u0000 \u0000S\u0000e\u0000n\u0000s\u0000o\u0000r\u0000 \u0000F\u0000u\u0000s\u0000i\u0000o\u0000n\u0000\r\u0000","sidebar":"tutorialSidebar"},"sensor-integration/touch-sensors":{"id":"sensor-integration/touch-sensors","title":"touch-sensors","description":"ÔøΩÔøΩ#\u0000 \u0000T\u0000o\u0000u\u0000c\u0000h\u0000 \u0000S\u0000e\u0000n\u0000s\u0000o\u0000r\u0000s\u0000\r\u0000","sidebar":"tutorialSidebar"}}}}