{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/introduction/welcome","label":"Welcome to AI-Native Robotics","docId":"introduction/welcome","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/introduction/how-to-use-this-book","label":"How to Use This Book","docId":"introduction/how-to-use-this-book","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/introduction/safety-primer","label":"Safety First: A Robotics Primer","docId":"introduction/safety-primer","unlisted":false}]},{"type":"category","label":"robotics-foundations","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"intro-to-robotics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/intro-to-robotics/what-is-a-robot","label":"What is a Robot?","docId":"robotics-foundations/intro-to-robotics/what-is-a-robot","unlisted":false}]},{"type":"category","label":"the-robot-operating-system-ros-2","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","label":"7.1 What is ROS 2?","docId":"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/core-concepts","label":"7.2 Core Concepts: Nodes, Topics, and Messages","docId":"robotics-foundations/the-robot-operating-system-ros-2/core-concepts","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","label":"7.3 Synchronous Communication: Services and Actions","docId":"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","label":"7.4 Managing Complexity: Launch Files and Parameters","docId":"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","label":"7.5 Visualization and Debugging","docId":"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","label":"7.6 Building a ROS 2 Package","docId":"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/labs","label":"7.7 Labs","docId":"robotics-foundations/the-robot-operating-system-ros-2/labs","unlisted":false}]}]},{"type":"link","href":"/ai-humanoid-robotics-book/docs/intro","label":"Tutorial Intro","docId":"intro","unlisted":false},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-page","label":"Create a Page","docId":"tutorial-basics/create-a-page","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-document","label":"Create a Document","docId":"tutorial-basics/create-a-document","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-blog-post","label":"Create a Blog Post","docId":"tutorial-basics/create-a-blog-post","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-basics/markdown-features","label":"Markdown Features","docId":"tutorial-basics/markdown-features","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-basics/deploy-your-site","label":"Deploy your site","docId":"tutorial-basics/deploy-your-site","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-basics/congratulations","label":"Congratulations!","docId":"tutorial-basics/congratulations","unlisted":false}],"href":"/ai-humanoid-robotics-book/docs/category/tutorial---basics"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-extras/manage-docs-versions","label":"Manage Docs Versions","docId":"tutorial-extras/manage-docs-versions","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/tutorial-extras/translate-your-site","label":"Translate your site","docId":"tutorial-extras/translate-your-site","unlisted":false}],"href":"/ai-humanoid-robotics-book/docs/category/tutorial---extras"},{"type":"category","label":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"simulation-environments","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","label":"7.1 The Importance of Simulation","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","label":"7.2 Physics Simulation Concepts","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","label":"7.3 Sensor Simulation","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","label":"7.4 Unity Visualization Plan","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","label":"7.5 Labs: From URDF to a Simulated Robot","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","unlisted":false}]},{"type":"category","label":"mlops-for-robotics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","label":"7.1 Introduction to the NVIDIA Isaac Platform","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","label":"7.2 The Isaac Sim Pipeline","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","label":"7.3 Synthetic Data Generation (SDG)","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","label":"7.4 Domain Randomization","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","label":"7.4 VSLAM using Isaac ROS","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","label":"7.5 Nav2 Path Planning Solutions","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","label":"7.5 Reinforcement Learning in Isaac Sim","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","label":"7.6 GPU Requirements","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","label":"7.6 Sim2Real: From Simulation to the Real World","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","label":"7.7 Labs and Benchmarks","docId":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","unlisted":false}]}]},{"type":"category","label":"labs-and-exercises","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/overview","label":"Module 5: Labs and Exercises","docId":"labs-and-exercises/overview","unlisted":false},{"type":"category","label":"vla-capstone-project","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/introduction","label":"8.1 The VLA Capstone Project","docId":"labs-and-exercises/vla-capstone-project/introduction","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/pipeline","label":"8.2 The Whisper → LLM → ROS 2 Pipeline","docId":"labs-and-exercises/vla-capstone-project/pipeline","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning","label":"8.3 Cognitive Planning: Natural Language to Actions","docId":"labs-and-exercises/vla-capstone-project/cognitive-planning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","label":"8.4 Object Detection and Manipulation","docId":"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","unlisted":false}]},{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-01-isaac-sim-hello-world","label":"Lab 1: Isaac Sim - Hello World","docId":"labs-and-exercises/lab-01-isaac-sim-hello-world","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-02-controlling-a-robot","label":"Lab 2: Controlling a Robot","docId":"labs-and-exercises/lab-02-controlling-a-robot","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-03-reinforcement-learning-reach-task","label":"Lab 3: Reinforcement Learning - Reach Task","docId":"labs-and-exercises/lab-03-reinforcement-learning-reach-task","unlisted":false}]}]},"docs":{"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","title":"7.4 Domain Randomization","description":"One of the most powerful techniques for training robust models with synthetic data is Domain Randomization. The goal is to expose the model to a wide variety of scenarios during training so that it can generalize well to the real world, which is inherently unpredictable.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","title":"7.6 GPU Requirements","description":"The NVIDIA Isaac platform is incredibly powerful, but that power comes with specific hardware requirements. Unlike other ROS 2 tools that are primarily CPU-bound, Isaac Sim and Isaac ROS are fundamentally GPU-bound. The performance of your simulation and AI workloads will be directly tied to the capabilities of your NVIDIA graphics card.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","title":"7.1 Introduction to the NVIDIA Isaac Platform","description":"While Gazebo provides a powerful, open-source foundation for robotics simulation, NVIDIA Isaac is a comprehensive, enterprise-grade platform designed to accelerate the development and deployment of AI-powered robots. It's an ecosystem of tools that leverages NVIDIA's expertise in GPU technology to tackle the most demanding tasks in robotics, from photorealistic simulation to hardware-accelerated AI perception.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","title":"7.2 The Isaac Sim Pipeline","description":"Understanding the workflow and core components of Isaac Sim is key to leveraging its power. The pipeline is built on the NVIDIA Omniverse platform, which uses a unique set of technologies for describing, simulating, and rendering complex 3D worlds.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","title":"7.7 Labs and Benchmarks","description":"These labs will provide hands-on experience with the core components of the NVIDIA Isaac ecosystem. Please ensure your system meets the GPU requirements outlined in the previous section before proceeding.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","title":"7.5 Nav2 Path Planning Solutions","description":"The ultimate goal of simulation is to develop and test autonomous behaviors that can be deployed on a physical robot. The standard tool for robot navigation in the ROS 2 ecosystem is the Nav2 stack. A key advantage of the Isaac Sim platform is its seamless integration with Nav2, enabling a true sim-to-real workflow.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","title":"7.5 Reinforcement Learning in Isaac Sim","description":"Isaac Sim is purpose-built for training reinforcement learning (RL) agents. Its ability to run thousands of parallel simulations on a single GPU makes it possible to collect massive amounts of training data in a short amount of time.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","title":"7.6 Sim2Real: From Simulation to the Real World","description":"The ultimate goal of training in simulation is to deploy the learned policy on a physical robot. This process is known as Sim2Real transfer. A successful Sim2Real transfer is the holy grail of robotics simulation, and Isaac Sim is designed to make this process as smooth as possible.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","title":"7.3 Synthetic Data Generation (SDG)","description":"One of the most compelling reasons to use a photorealistic simulator like Isaac Sim is for Synthetic Data Generation (SDG). Training modern perception models, especially for object detection and segmentation, requires enormous, hand-labeled datasets. SDG automates this process, allowing you to generate perfectly labeled data at a massive scale.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","title":"7.4 VSLAM using Isaac ROS","description":"While Isaac Sim provides the world, Isaac ROS provides the robot's brain. Isaac ROS is a collection of high-performance ROS 2 packages, or Gems, that are hardware-accelerated to run on NVIDIA GPUs and Jetson devices. These Gems provide optimized implementations of common and computationally expensive robotics algorithms.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","title":"7.1 The Importance of Simulation","description":"In robotics, the cost of a mistake can be high. A software bug that might crash a web server could cause a physical robot to collide with an obstacle, damage itself, or pose a safety risk. This is why simulation is one of the most critical tools in the modern robotics development lifecycle.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","title":"7.5 Labs: From URDF to a Simulated Robot","description":"These labs will guide you through the process of creating a robot model, adding dynamics to it, and controlling it in the Gazebo simulator.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","title":"7.2 Physics Simulation Concepts","description":"To create a valuable Digital Twin, the simulation must be more than just a pretty picture; it must be physically plausible. Gazebo uses high-performance physics engines (like ODE or Bullet) to compute the effects of forces and collisions on your robot model, allowing you to test its dynamics in a realistic way.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","title":"7.3 Sensor Simulation","description":"One of the most powerful features of Gazebo is its ability to simulate a wide range of robotic sensors. The data generated by these virtual sensors is published to ROS 2 topics, just like data from real hardware. This allows you to test your entire perception and control stack in the simulation without needing a physical robot.","sidebar":"tutorialSidebar"},"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan":{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","title":"7.4 Unity Visualization Plan","description":"While Gazebo is a powerful tool for physics simulation, its built-in rendering engine is optimized for speed, not photorealism. For applications that require high-fidelity graphics—such as generating synthetic data for AI, creating marketing materials, or developing user interfaces—it's common to pair Gazebo with a modern game engine like Unity.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Tutorial Intro","description":"Let's discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"introduction/how-to-use-this-book":{"id":"introduction/how-to-use-this-book","title":"How to Use This Book","description":"","sidebar":"tutorialSidebar"},"introduction/safety-primer":{"id":"introduction/safety-primer","title":"Safety First: A Robotics Primer","description":"","sidebar":"tutorialSidebar"},"introduction/welcome":{"id":"introduction/welcome","title":"Welcome to AI-Native Robotics","description":"","sidebar":"tutorialSidebar"},"labs-and-exercises/lab-01-isaac-sim-hello-world":{"id":"labs-and-exercises/lab-01-isaac-sim-hello-world","title":"Lab 1: Isaac Sim - Hello World","description":"Goal: Create a simple \"Hello World\" scene in Isaac Sim to get familiar with the interface and basic concepts.","sidebar":"tutorialSidebar"},"labs-and-exercises/lab-02-controlling-a-robot":{"id":"labs-and-exercises/lab-02-controlling-a-robot","title":"Lab 2: Controlling a Robot","description":"Goal: Add a Franka Emika Panda robot to the scene and control its joints.","sidebar":"tutorialSidebar"},"labs-and-exercises/lab-03-reinforcement-learning-reach-task":{"id":"labs-and-exercises/lab-03-reinforcement-learning-reach-task","title":"Lab 3: Reinforcement Learning - Reach Task","description":"Goal: Train a Franka robot to reach a target using reinforcement learning.","sidebar":"tutorialSidebar"},"labs-and-exercises/overview":{"id":"labs-and-exercises/overview","title":"Module 5: Labs and Exercises","description":"Welcome to the hands-on portion of this book! The following labs and exercises are designed to solidify your understanding of the concepts we've discussed. Each lab will provide you with a practical problem to solve, leveraging the tools and techniques covered in the previous modules.","sidebar":"tutorialSidebar"},"labs-and-exercises/vla-capstone-project/cognitive-planning":{"id":"labs-and-exercises/vla-capstone-project/cognitive-planning","title":"8.3 Cognitive Planning: Natural Language to Actions","description":"The most innovative part of our VLA architecture is the use of a Large Language Model (LLM) as a high-level task planner. The LLM's vast knowledge of language and reasoning allows it to bridge the ambiguous, flexible world of human commands with the structured, precise world of robot operations.","sidebar":"tutorialSidebar"},"labs-and-exercises/vla-capstone-project/introduction":{"id":"labs-and-exercises/vla-capstone-project/introduction","title":"8.1 The VLA Capstone Project","description":"Welcome to the final capstone project. This module integrates the key concepts from the entire book—robotics foundations, hardware, AI control, and simulation—into a single, ambitious project: building a Vision-Language-Action (VLA) system.","sidebar":"tutorialSidebar"},"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation":{"id":"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","title":"8.4 Object Detection and Manipulation","description":"For our robot to act on the world, it needs two key capabilities: perception to understand its environment and manipulation to interact with it. In our VLA system, these are handled by dedicated ROS 2 nodes that provide their functionality as services to the main planner.","sidebar":"tutorialSidebar"},"labs-and-exercises/vla-capstone-project/pipeline":{"id":"labs-and-exercises/vla-capstone-project/pipeline","title":"8.2 The Whisper → LLM → ROS 2 Pipeline","description":"The core of our VLA system is the data flow that converts human speech into a series of executable robot commands. This pipeline consists of three main stages, each handled by a dedicated ROS 2 node.","sidebar":"tutorialSidebar"},"robotics-foundations/intro-to-robotics/what-is-a-robot":{"id":"robotics-foundations/intro-to-robotics/what-is-a-robot","title":"What is a Robot?","description":"","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package":{"id":"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","title":"7.6 Building a ROS 2 Package","description":"So far, we've focused on writing and running individual nodes. However, the real power of ROS 2 comes from organizing your code into reusable, shareable units called packages. A package is a directory containing your nodes, launch files, custom message definitions, and a set of files that describe its contents and dependencies.","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/core-concepts":{"id":"robotics-foundations/the-robot-operating-system-ros-2/core-concepts","title":"7.2 Core Concepts: Nodes, Topics, and Messages","description":"At the heart of ROS 2 is a communication graph where independent programs exchange information. Understanding the three fundamental components of this graph—Nodes, Topics, and Messages—is the key to mastering ROS 2.","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/labs":{"id":"robotics-foundations/the-robot-operating-system-ros-2/labs","title":"7.7 Labs","description":"These labs will guide you through the practical steps of creating, building, and running a multi-node ROS 2 application. Following these exercises will solidify the core concepts discussed in this chapter.","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity":{"id":"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","title":"7.4 Managing Complexity: Launch Files and Parameters","description":"As your robotics application grows from two nodes to ten, twenty, or even a hundred, starting and configuring each one manually becomes impossible. This is where ROS 2's launch system comes in. Launch files are powerful scripts that allow you to start, configure, and connect a complex system of nodes with a single command.","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication":{"id":"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","title":"7.3 Synchronous Communication: Services and Actions","description":"While topics are perfect for continuous data streams, sometimes you need a more direct, synchronous form of communication. For this, ROS 2 provides two powerful mechanisms: Services for quick request/response interactions, and Actions for long-running tasks that require feedback.","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging":{"id":"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","title":"7.5 Visualization and Debugging","description":"A running ROS 2 system is a complex, distributed network of nodes passing messages. Without the right tools, understanding what's happening can be nearly impossible. Fortunately, ROS 2 comes with a powerful suite of command-line and graphical tools for introspection, debugging, and visualization.","sidebar":"tutorialSidebar"},"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2":{"id":"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","title":"7.1 What is ROS 2?","description":"Welcome to the Robot Operating System (ROS 2), the communication backbone of modern robotics. While not a traditional operating system like Windows or Linux, ROS 2 provides a powerful framework of libraries and tools to help you build complex robot applications. Think of it less as an OS and more as a robotic nervous system—a standardized way for different parts of your robot to communicate and work together.","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"tutorialSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"tutorialSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let's translate docs/intro.md to French.","sidebar":"tutorialSidebar"}}}}