{"allContent":{"docusaurus-plugin-css-cascade-layers":{},"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/ai-humanoid-robotics-book/docs","tagsPath":"/ai-humanoid-robotics-book/docs/tags","editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\SHAYAN\\Desktop\\hackathon\\ai-humanoid-robotics-book\\sidebars.ts","contentPath":"C:\\Users\\SHAYAN\\Desktop\\hackathon\\ai-humanoid-robotics-book\\docs","docs":[{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","title":"7.4 Domain Randomization","description":"One of the most powerful techniques for training robust models with synthetic data is Domain Randomization. The goal is to expose the model to a wide variety of scenarios during training so that it can generalize well to the real world, which is inherently unpredictable.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/04-domain-randomization.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/04-domain-randomization.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"7.3 Synthetic Data Generation (SDG)","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation"},"next":{"title":"7.4 VSLAM using Isaac ROS","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","title":"7.6 GPU Requirements","description":"The NVIDIA Isaac platform is incredibly powerful, but that power comes with specific hardware requirements. Unlike other ROS 2 tools that are primarily CPU-bound, Isaac Sim and Isaac ROS are fundamentally GPU-bound. The performance of your simulation and AI workloads will be directly tied to the capabilities of your NVIDIA graphics card.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/06-gpu-requirements.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/06-gpu-requirements.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"7.5 Reinforcement Learning in Isaac Sim","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning"},"next":{"title":"7.6 Sim2Real: From Simulation to the Real World","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","title":"7.1 Introduction to the NVIDIA Isaac Platform","description":"While Gazebo provides a powerful, open-source foundation for robotics simulation, NVIDIA Isaac is a comprehensive, enterprise-grade platform designed to accelerate the development and deployment of AI-powered robots. It's an ecosystem of tools that leverages NVIDIA's expertise in GPU technology to tackle the most demanding tasks in robotics, from photorealistic simulation to hardware-accelerated AI perception.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/01-introduction-to-nvidia-isaac.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/01-introduction-to-nvidia-isaac.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"7.5 Labs: From URDF to a Simulated Robot","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs"},"next":{"title":"7.2 The Isaac Sim Pipeline","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","title":"7.2 The Isaac Sim Pipeline","description":"Understanding the workflow and core components of Isaac Sim is key to leveraging its power. The pipeline is built on the NVIDIA Omniverse platform, which uses a unique set of technologies for describing, simulating, and rendering complex 3D worlds.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/02-isaac-sim-pipeline.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/02-isaac-sim-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"7.1 Introduction to the NVIDIA Isaac Platform","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac"},"next":{"title":"7.3 Synthetic Data Generation (SDG)","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","title":"7.7 Labs and Benchmarks","description":"These labs will provide hands-on experience with the core components of the NVIDIA Isaac ecosystem. Please ensure your system meets the GPU requirements outlined in the previous section before proceeding.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/07-labs-and-benchmarks.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/07-labs-and-benchmarks.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"7.6 Sim2Real: From Simulation to the Real World","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real"},"next":{"title":"Module 5: Labs and Exercises","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/overview"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","title":"7.5 Nav2 Path Planning Solutions","description":"The ultimate goal of simulation is to develop and test autonomous behaviors that can be deployed on a physical robot. The standard tool for robot navigation in the ROS 2 ecosystem is the Nav2 stack. A key advantage of the Isaac Sim platform is its seamless integration with Nav2, enabling a true sim-to-real workflow.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/05-nav2-path-planning.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/05-nav2-path-planning.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"7.4 VSLAM using Isaac ROS","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros"},"next":{"title":"7.5 Reinforcement Learning in Isaac Sim","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","title":"7.5 Reinforcement Learning in Isaac Sim","description":"Isaac Sim is purpose-built for training reinforcement learning (RL) agents. Its ability to run thousands of parallel simulations on a single GPU makes it possible to collect massive amounts of training data in a short amount of time.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/05-reinforcement-learning.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/05-reinforcement-learning.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"7.5 Nav2 Path Planning Solutions","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning"},"next":{"title":"7.6 GPU Requirements","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","title":"7.6 Sim2Real: From Simulation to the Real World","description":"The ultimate goal of training in simulation is to deploy the learned policy on a physical robot. This process is known as Sim2Real transfer. A successful Sim2Real transfer is the holy grail of robotics simulation, and Isaac Sim is designed to make this process as smooth as possible.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/06-sim2real.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/06-sim2real.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"7.6 GPU Requirements","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements"},"next":{"title":"7.7 Labs and Benchmarks","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","title":"7.3 Synthetic Data Generation (SDG)","description":"One of the most compelling reasons to use a photorealistic simulator like Isaac Sim is for Synthetic Data Generation (SDG). Training modern perception models, especially for object detection and segmentation, requires enormous, hand-labeled datasets. SDG automates this process, allowing you to generate perfectly labeled data at a massive scale.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/03-synthetic-data-generation.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/03-synthetic-data-generation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"7.2 The Isaac Sim Pipeline","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline"},"next":{"title":"7.4 Domain Randomization","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","title":"7.4 VSLAM using Isaac ROS","description":"While Isaac Sim provides the world, Isaac ROS provides the robot's brain. Isaac ROS is a collection of high-performance ROS 2 packages, or Gems, that are hardware-accelerated to run on NVIDIA GPUs and Jetson devices. These Gems provide optimized implementations of common and computationally expensive robotics algorithms.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/04-vslam-with-isaac-ros.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/08-mlops-for-robotics/04-vslam-with-isaac-ros.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"7.4 Domain Randomization","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization"},"next":{"title":"7.5 Nav2 Path Planning Solutions","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","title":"7.1 The Importance of Simulation","description":"In robotics, the cost of a mistake can be high. A software bug that might crash a web server could cause a physical robot to collide with an obstacle, damage itself, or pose a safety risk. This is why simulation is one of the most critical tools in the modern robotics development lifecycle.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/01-importance-of-simulation.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/01-importance-of-simulation.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Translate your site","permalink":"/ai-humanoid-robotics-book/docs/tutorial-extras/translate-your-site"},"next":{"title":"7.2 Physics Simulation Concepts","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","title":"7.5 Labs: From URDF to a Simulated Robot","description":"These labs will guide you through the process of creating a robot model, adding dynamics to it, and controlling it in the Gazebo simulator.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/05-labs.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/05-labs.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"7.4 Unity Visualization Plan","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan"},"next":{"title":"7.1 Introduction to the NVIDIA Isaac Platform","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","title":"7.2 Physics Simulation Concepts","description":"To create a valuable Digital Twin, the simulation must be more than just a pretty picture; it must be physically plausible. Gazebo uses high-performance physics engines (like ODE or Bullet) to compute the effects of forces and collisions on your robot model, allowing you to test its dynamics in a realistic way.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/02-physics-simulation-concepts.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/02-physics-simulation-concepts.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"7.1 The Importance of Simulation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation"},"next":{"title":"7.3 Sensor Simulation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","title":"7.3 Sensor Simulation","description":"One of the most powerful features of Gazebo is its ability to simulate a wide range of robotic sensors. The data generated by these virtual sensors is published to ROS 2 topics, just like data from real hardware. This allows you to test your entire perception and control stack in the simulation without needing a physical robot.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/03-sensor-simulation.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/03-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"7.2 Physics Simulation Concepts","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts"},"next":{"title":"7.4 Unity Visualization Plan","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan"}},{"id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","title":"7.4 Unity Visualization Plan","description":"While Gazebo is a powerful tool for physics simulation, its built-in rendering engine is optimized for speed, not photorealism. For applications that require high-fidelity graphics—such as generating synthetic data for AI, creating marketing materials, or developing user interfaces—it's common to pair Gazebo with a modern game engine like Unity.","source":"@site/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/04-unity-visualization-plan.md","sourceDirName":"04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments","slug":"/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/04-autonomous-behaviors-multi-modal-sensors-and-cloud-ops/07-simulation-environments/04-unity-visualization-plan.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"7.3 Sensor Simulation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation"},"next":{"title":"7.5 Labs: From URDF to a Simulated Robot","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs"}},{"id":"intro","title":"Tutorial Intro","description":"Let's discover Docusaurus in less than 5 minutes.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/ai-humanoid-robotics-book/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"7.7 Labs","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/labs"},"next":{"title":"Tutorial - Basics","permalink":"/ai-humanoid-robotics-book/docs/category/tutorial---basics"}},{"id":"introduction/how-to-use-this-book","title":"How to Use This Book","description":"","source":"@site/docs/00-introduction/02-how-to-use-this-book.mdx","sourceDirName":"00-introduction","slug":"/introduction/how-to-use-this-book","permalink":"/ai-humanoid-robotics-book/docs/introduction/how-to-use-this-book","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/00-introduction/02-how-to-use-this-book.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Welcome to AI-Native Robotics","permalink":"/ai-humanoid-robotics-book/docs/introduction/welcome"},"next":{"title":"Safety First: A Robotics Primer","permalink":"/ai-humanoid-robotics-book/docs/introduction/safety-primer"}},{"id":"introduction/safety-primer","title":"Safety First: A Robotics Primer","description":"","source":"@site/docs/00-introduction/03-safety-primer.mdx","sourceDirName":"00-introduction","slug":"/introduction/safety-primer","permalink":"/ai-humanoid-robotics-book/docs/introduction/safety-primer","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/00-introduction/03-safety-primer.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"How to Use This Book","permalink":"/ai-humanoid-robotics-book/docs/introduction/how-to-use-this-book"},"next":{"title":"What is a Robot?","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/intro-to-robotics/what-is-a-robot"}},{"id":"introduction/welcome","title":"Welcome to AI-Native Robotics","description":"","source":"@site/docs/00-introduction/01-welcome.mdx","sourceDirName":"00-introduction","slug":"/introduction/welcome","permalink":"/ai-humanoid-robotics-book/docs/introduction/welcome","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/00-introduction/01-welcome.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","next":{"title":"How to Use This Book","permalink":"/ai-humanoid-robotics-book/docs/introduction/how-to-use-this-book"}},{"id":"labs-and-exercises/lab-01-isaac-sim-hello-world","title":"Lab 1: Isaac Sim - Hello World","description":"Goal: Create a simple \"Hello World\" scene in Isaac Sim to get familiar with the interface and basic concepts.","source":"@site/docs/05-labs-and-exercises/02-lab-01-isaac-sim-hello-world.md","sourceDirName":"05-labs-and-exercises","slug":"/labs-and-exercises/lab-01-isaac-sim-hello-world","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-01-isaac-sim-hello-world","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/02-lab-01-isaac-sim-hello-world.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"8.4 Object Detection and Manipulation","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation"},"next":{"title":"Lab 2: Controlling a Robot","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-02-controlling-a-robot"}},{"id":"labs-and-exercises/lab-02-controlling-a-robot","title":"Lab 2: Controlling a Robot","description":"Goal: Add a Franka Emika Panda robot to the scene and control its joints.","source":"@site/docs/05-labs-and-exercises/03-lab-02-controlling-a-robot.md","sourceDirName":"05-labs-and-exercises","slug":"/labs-and-exercises/lab-02-controlling-a-robot","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-02-controlling-a-robot","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/03-lab-02-controlling-a-robot.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Lab 1: Isaac Sim - Hello World","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-01-isaac-sim-hello-world"},"next":{"title":"Lab 3: Reinforcement Learning - Reach Task","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-03-reinforcement-learning-reach-task"}},{"id":"labs-and-exercises/lab-03-reinforcement-learning-reach-task","title":"Lab 3: Reinforcement Learning - Reach Task","description":"Goal: Train a Franka robot to reach a target using reinforcement learning.","source":"@site/docs/05-labs-and-exercises/04-lab-03-reinforcement-learning-reach-task.md","sourceDirName":"05-labs-and-exercises","slug":"/labs-and-exercises/lab-03-reinforcement-learning-reach-task","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-03-reinforcement-learning-reach-task","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/04-lab-03-reinforcement-learning-reach-task.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lab 2: Controlling a Robot","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-02-controlling-a-robot"}},{"id":"labs-and-exercises/overview","title":"Module 5: Labs and Exercises","description":"Welcome to the hands-on portion of this book! The following labs and exercises are designed to solidify your understanding of the concepts we've discussed. Each lab will provide you with a practical problem to solve, leveraging the tools and techniques covered in the previous modules.","source":"@site/docs/05-labs-and-exercises/01-overview.md","sourceDirName":"05-labs-and-exercises","slug":"/labs-and-exercises/overview","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"7.7 Labs and Benchmarks","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks"},"next":{"title":"8.1 The VLA Capstone Project","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/introduction"}},{"id":"labs-and-exercises/vla-capstone-project/cognitive-planning","title":"8.3 Cognitive Planning: Natural Language to Actions","description":"The most innovative part of our VLA architecture is the use of a Large Language Model (LLM) as a high-level task planner. The LLM's vast knowledge of language and reasoning allows it to bridge the ambiguous, flexible world of human commands with the structured, precise world of robot operations.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/03-cognitive-planning.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/cognitive-planning","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/03-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"8.2 The Whisper → LLM → ROS 2 Pipeline","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/pipeline"},"next":{"title":"8.4 Object Detection and Manipulation","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation"}},{"id":"labs-and-exercises/vla-capstone-project/introduction","title":"8.1 The VLA Capstone Project","description":"Welcome to the final capstone project. This module integrates the key concepts from the entire book—robotics foundations, hardware, AI control, and simulation—into a single, ambitious project: building a Vision-Language-Action (VLA) system.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/01-introduction.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/introduction","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 5: Labs and Exercises","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/overview"},"next":{"title":"8.2 The Whisper → LLM → ROS 2 Pipeline","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/pipeline"}},{"id":"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","title":"8.4 Object Detection and Manipulation","description":"For our robot to act on the world, it needs two key capabilities: perception to understand its environment and manipulation to interact with it. In our VLA system, these are handled by dedicated ROS 2 nodes that provide their functionality as services to the main planner.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/04-object-detection-and-manipulation.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/04-object-detection-and-manipulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"8.3 Cognitive Planning: Natural Language to Actions","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning"},"next":{"title":"Lab 1: Isaac Sim - Hello World","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-01-isaac-sim-hello-world"}},{"id":"labs-and-exercises/vla-capstone-project/pipeline","title":"8.2 The Whisper → LLM → ROS 2 Pipeline","description":"The core of our VLA system is the data flow that converts human speech into a series of executable robot commands. This pipeline consists of three main stages, each handled by a dedicated ROS 2 node.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/02-pipeline.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/pipeline","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/02-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"8.1 The VLA Capstone Project","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/introduction"},"next":{"title":"8.3 Cognitive Planning: Natural Language to Actions","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning"}},{"id":"robotics-foundations/intro-to-robotics/what-is-a-robot","title":"What is a Robot?","description":"","source":"@site/docs/01-robotics-foundations/01-intro-to-robotics/01-what-is-a-robot.mdx","sourceDirName":"01-robotics-foundations/01-intro-to-robotics","slug":"/robotics-foundations/intro-to-robotics/what-is-a-robot","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/intro-to-robotics/what-is-a-robot","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/01-intro-to-robotics/01-what-is-a-robot.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Safety First: A Robotics Primer","permalink":"/ai-humanoid-robotics-book/docs/introduction/safety-primer"},"next":{"title":"7.1 What is ROS 2?","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","title":"7.6 Building a ROS 2 Package","description":"So far, we've focused on writing and running individual nodes. However, the real power of ROS 2 comes from organizing your code into reusable, shareable units called packages. A package is a directory containing your nodes, launch files, custom message definitions, and a set of files that describe its contents and dependencies.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/06-building-a-ros-2-package.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/06-building-a-ros-2-package.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"7.5 Visualization and Debugging","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging"},"next":{"title":"7.7 Labs","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/labs"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/core-concepts","title":"7.2 Core Concepts: Nodes, Topics, and Messages","description":"At the heart of ROS 2 is a communication graph where independent programs exchange information. Understanding the three fundamental components of this graph—Nodes, Topics, and Messages—is the key to mastering ROS 2.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/02-core-concepts.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/core-concepts","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/core-concepts","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/02-core-concepts.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"7.1 What is ROS 2?","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2"},"next":{"title":"7.3 Synchronous Communication: Services and Actions","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/labs","title":"7.7 Labs","description":"These labs will guide you through the practical steps of creating, building, and running a multi-node ROS 2 application. Following these exercises will solidify the core concepts discussed in this chapter.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/07-labs.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/labs","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/labs","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/07-labs.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"7.6 Building a ROS 2 Package","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package"},"next":{"title":"Tutorial Intro","permalink":"/ai-humanoid-robotics-book/docs/intro"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","title":"7.4 Managing Complexity: Launch Files and Parameters","description":"As your robotics application grows from two nodes to ten, twenty, or even a hundred, starting and configuring each one manually becomes impossible. This is where ROS 2's launch system comes in. Launch files are powerful scripts that allow you to start, configure, and connect a complex system of nodes with a single command.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/04-managing-complexity.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/managing-complexity","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/04-managing-complexity.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"7.3 Synchronous Communication: Services and Actions","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication"},"next":{"title":"7.5 Visualization and Debugging","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","title":"7.3 Synchronous Communication: Services and Actions","description":"While topics are perfect for continuous data streams, sometimes you need a more direct, synchronous form of communication. For this, ROS 2 provides two powerful mechanisms: Services for quick request/response interactions, and Actions for long-running tasks that require feedback.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/03-synchronous-communication.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/03-synchronous-communication.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"7.2 Core Concepts: Nodes, Topics, and Messages","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/core-concepts"},"next":{"title":"7.4 Managing Complexity: Launch Files and Parameters","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/managing-complexity"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","title":"7.5 Visualization and Debugging","description":"A running ROS 2 system is a complex, distributed network of nodes passing messages. Without the right tools, understanding what's happening can be nearly impossible. Fortunately, ROS 2 comes with a powerful suite of command-line and graphical tools for introspection, debugging, and visualization.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/05-visualization-and-debugging.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/05-visualization-and-debugging.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"7.4 Managing Complexity: Launch Files and Parameters","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/managing-complexity"},"next":{"title":"7.6 Building a ROS 2 Package","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package"}},{"id":"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","title":"7.1 What is ROS 2?","description":"Welcome to the Robot Operating System (ROS 2), the communication backbone of modern robotics. While not a traditional operating system like Windows or Linux, ROS 2 provides a powerful framework of libraries and tools to help you build complex robot applications. Think of it less as an OS and more as a robotic nervous system—a standardized way for different parts of your robot to communicate and work together.","source":"@site/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/01-what-is-ros-2.md","sourceDirName":"01-robotics-foundations/07-the-robot-operating-system-ros-2","slug":"/robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/01-robotics-foundations/07-the-robot-operating-system-ros-2/01-what-is-ros-2.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"What is a Robot?","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/intro-to-robotics/what-is-a-robot"},"next":{"title":"7.2 Core Concepts: Nodes, Topics, and Messages","permalink":"/ai-humanoid-robotics-book/docs/robotics-foundations/the-robot-operating-system-ros-2/core-concepts"}},{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","source":"@site/docs/tutorial-basics/congratulations.md","sourceDirName":"tutorial-basics","slug":"/tutorial-basics/congratulations","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/congratulations","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-basics/congratulations.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Deploy your site","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/deploy-your-site"},"next":{"title":"Tutorial - Extras","permalink":"/ai-humanoid-robotics-book/docs/category/tutorial---extras"}},{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","source":"@site/docs/tutorial-basics/create-a-blog-post.md","sourceDirName":"tutorial-basics","slug":"/tutorial-basics/create-a-blog-post","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-blog-post","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-basics/create-a-blog-post.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Create a Document","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-document"},"next":{"title":"Markdown Features","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/markdown-features"}},{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","source":"@site/docs/tutorial-basics/create-a-document.md","sourceDirName":"tutorial-basics","slug":"/tutorial-basics/create-a-document","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-document","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-basics/create-a-document.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Create a Page","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-page"},"next":{"title":"Create a Blog Post","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-blog-post"}},{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","source":"@site/docs/tutorial-basics/create-a-page.md","sourceDirName":"tutorial-basics","slug":"/tutorial-basics/create-a-page","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-page","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-basics/create-a-page.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Tutorial - Basics","permalink":"/ai-humanoid-robotics-book/docs/category/tutorial---basics"},"next":{"title":"Create a Document","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-document"}},{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","source":"@site/docs/tutorial-basics/deploy-your-site.md","sourceDirName":"tutorial-basics","slug":"/tutorial-basics/deploy-your-site","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/deploy-your-site","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-basics/deploy-your-site.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Markdown Features","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/markdown-features"},"next":{"title":"Congratulations!","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/congratulations"}},{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","source":"@site/docs/tutorial-basics/markdown-features.mdx","sourceDirName":"tutorial-basics","slug":"/tutorial-basics/markdown-features","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/markdown-features","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-basics/markdown-features.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Create a Blog Post","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/create-a-blog-post"},"next":{"title":"Deploy your site","permalink":"/ai-humanoid-robotics-book/docs/tutorial-basics/deploy-your-site"}},{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","source":"@site/docs/tutorial-extras/manage-docs-versions.md","sourceDirName":"tutorial-extras","slug":"/tutorial-extras/manage-docs-versions","permalink":"/ai-humanoid-robotics-book/docs/tutorial-extras/manage-docs-versions","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-extras/manage-docs-versions.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Tutorial - Extras","permalink":"/ai-humanoid-robotics-book/docs/category/tutorial---extras"},"next":{"title":"Translate your site","permalink":"/ai-humanoid-robotics-book/docs/tutorial-extras/translate-your-site"}},{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let's translate docs/intro.md to French.","source":"@site/docs/tutorial-extras/translate-your-site.md","sourceDirName":"tutorial-extras","slug":"/tutorial-extras/translate-your-site","permalink":"/ai-humanoid-robotics-book/docs/tutorial-extras/translate-your-site","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/tutorial-extras/translate-your-site.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Manage Docs Versions","permalink":"/ai-humanoid-robotics-book/docs/tutorial-extras/manage-docs-versions"},"next":{"title":"7.1 The Importance of Simulation","permalink":"/ai-humanoid-robotics-book/docs/autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"category","label":"introduction","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"introduction/welcome"},{"type":"doc","id":"introduction/how-to-use-this-book"},{"type":"doc","id":"introduction/safety-primer"}]},{"type":"category","label":"robotics-foundations","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"intro-to-robotics","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"robotics-foundations/intro-to-robotics/what-is-a-robot"}]},{"type":"category","label":"the-robot-operating-system-ros-2","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/what-is-ros-2"},{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/core-concepts"},{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/synchronous-communication"},{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/managing-complexity"},{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/visualization-and-debugging"},{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/building-a-ros-2-package"},{"type":"doc","id":"robotics-foundations/the-robot-operating-system-ros-2/labs"}]}]},{"type":"doc","id":"intro"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"tutorial-basics/create-a-page"},{"type":"doc","id":"tutorial-basics/create-a-document"},{"type":"doc","id":"tutorial-basics/create-a-blog-post"},{"type":"doc","id":"tutorial-basics/markdown-features"},{"type":"doc","id":"tutorial-basics/deploy-your-site"},{"type":"doc","id":"tutorial-basics/congratulations"}],"link":{"type":"generated-index","description":"5 minutes to learn the most important Docusaurus concepts.","slug":"/category/tutorial---basics","permalink":"/ai-humanoid-robotics-book/docs/category/tutorial---basics"}},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"tutorial-extras/manage-docs-versions"},{"type":"doc","id":"tutorial-extras/translate-your-site"}],"link":{"type":"generated-index","slug":"/category/tutorial---extras","permalink":"/ai-humanoid-robotics-book/docs/category/tutorial---extras"}},{"type":"category","label":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"simulation-environments","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/importance-of-simulation"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/physics-simulation-concepts"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/sensor-simulation"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/unity-visualization-plan"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/simulation-environments/labs"}]},{"type":"category","label":"mlops-for-robotics","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/introduction-to-nvidia-isaac"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/isaac-sim-pipeline"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/synthetic-data-generation"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/domain-randomization"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/vslam-with-isaac-ros"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/nav2-path-planning"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/reinforcement-learning"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/gpu-requirements"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/sim2real"},{"type":"doc","id":"autonomous-behaviors-multi-modal-sensors-and-cloud-ops/mlops-for-robotics/labs-and-benchmarks"}]}]},{"type":"category","label":"labs-and-exercises","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"labs-and-exercises/overview"},{"type":"category","label":"vla-capstone-project","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"labs-and-exercises/vla-capstone-project/introduction"},{"type":"doc","id":"labs-and-exercises/vla-capstone-project/pipeline"},{"type":"doc","id":"labs-and-exercises/vla-capstone-project/cognitive-planning"},{"type":"doc","id":"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation"}]},{"type":"doc","id":"labs-and-exercises/lab-01-isaac-sim-hello-world"},{"type":"doc","id":"labs-and-exercises/lab-02-controlling-a-robot"},{"type":"doc","id":"labs-and-exercises/lab-03-reinforcement-learning-reach-task"}]}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/ai-humanoid-robotics-book/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/ai-humanoid-robotics-book/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}