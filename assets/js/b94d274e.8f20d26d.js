"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[6460],{7509:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","title":"8.4 Object Detection and Manipulation","description":"For our robot to act on the world, it needs two key capabilities: perception to understand its environment and manipulation to interact with it. In our VLA system, these are handled by dedicated ROS 2 nodes that provide their functionality as services to the main planner.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/04-object-detection-and-manipulation.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/04-object-detection-and-manipulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"8.3 Cognitive Planning: Natural Language to Actions","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning"},"next":{"title":"Lab 1: Isaac Sim - Hello World","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-01-isaac-sim-hello-world"}}');var t=o(4848),s=o(8453);const r={sidebar_position:4},c="8.4 Object Detection and Manipulation",a={},l=[{value:"Vision System: Object Detection",id:"vision-system-object-detection",level:3},{value:"Manipulation System: Motion Planning",id:"manipulation-system-motion-planning",level:3},{value:"Gripper Control",id:"gripper-control",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"84-object-detection-and-manipulation",children:"8.4 Object Detection and Manipulation"})}),"\n",(0,t.jsxs)(n.p,{children:["For our robot to act on the world, it needs two key capabilities: ",(0,t.jsx)(n.strong,{children:"perception"})," to understand its environment and ",(0,t.jsx)(n.strong,{children:"manipulation"})," to interact with it. In our VLA system, these are handled by dedicated ROS 2 nodes that provide their functionality as services to the main planner."]}),"\n",(0,t.jsx)(n.h3,{id:"vision-system-object-detection",children:"Vision System: Object Detection"}),"\n",(0,t.jsxs)(n.p,{children:['The "find_object" skill in our planner needs a robust backend. This will be a ',(0,t.jsx)(n.code,{children:"VisionNode"})," responsible for identifying and locating objects in the robot's workspace."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input:"})," A continuous stream of images from a camera (simulated in our case)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Processing:"})," The node uses a pre-trained ",(0,t.jsx)(n.strong,{children:"object detection model"}),". A popular and effective choice is ",(0,t.jsx)(n.strong,{children:"YOLO (You Only Look Once)"}),", which is fast and accurate. The model will process each image and identify bounding boxes for all known objects."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3D Position:"})," To go from a 2D bounding box in an image to a 3D coordinate in the robot's world, we will use the camera's ",(0,t.jsx)(n.strong,{children:"depth sensor"}),". By finding the center of the bounding box and looking up the corresponding distance from the depth map, we can calculate the object's ",(0,t.jsx)(n.code,{children:"[x, y, z]"})," position relative to the camera."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Service:"})," The ",(0,t.jsx)(n.code,{children:"VisionNode"})," exposes its functionality through a ROS 2 service, for example, ",(0,t.jsx)(n.code,{children:"/vision/find_object"}),".","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request:"})," ",(0,t.jsx)(n.code,{children:"string object_name"}),' (e.g., "red cube")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response:"})," ",(0,t.jsx)(n.code,{children:"geometry_msgs/Point position"})," (the 3D coordinates) and ",(0,t.jsx)(n.code,{children:"bool success"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"manipulation-system-motion-planning",children:"Manipulation System: Motion Planning"}),"\n",(0,t.jsxs)(n.p,{children:["Once we have the coordinates of an object, we need to move the robot arm to it safely and efficiently. This is the job of the ",(0,t.jsx)(n.strong,{children:"manipulation"})," system, which will be powered by the ",(0,t.jsx)(n.strong,{children:"MoveIt2"})," framework."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MoveIt2:"})," MoveIt2 is the standard motion planning library in ROS 2. It takes a start state, a goal state (e.g., the 6D pose of the end-effector), and a description of the robot and its environment, and it computes a collision-free trajectory for the arm's joints."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node:"})," A ",(0,t.jsx)(n.code,{children:"MotionNode"})," will wrap the MoveIt2 functionality."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Service:"})," It will expose a service like ",(0,t.jsx)(n.code,{children:"/motion/move_to_pose"}),".","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request:"})," ",(0,t.jsx)(n.code,{children:"geometry_msgs/PoseStamped target_pose"})," (the desired position and orientation of the gripper)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response:"})," ",(0,t.jsx)(n.code,{children:"bool success"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"gripper-control",children:"Gripper Control"}),"\n",(0,t.jsx)(n.p,{children:"The final piece is the ability to grasp and release objects."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node:"})," A simple ",(0,t.jsx)(n.code,{children:"GripperNode"})," will control the robot's end-effector."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Service:"})," It will provide a service like ",(0,t.jsx)(n.code,{children:"/gripper/set_state"}),".","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request:"})," ",(0,t.jsx)(n.code,{children:"string state"}),' ("open" or "close").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response:"})," ",(0,t.jsx)(n.code,{children:"bool success"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["By encapsulating these complex tasks into discrete services, we create a clean and modular architecture. The main ",(0,t.jsx)(n.code,{children:"CognitivePlannerNode"})," doesn't need to know ",(0,t.jsx)(n.em,{children:"how"})," object detection or motion planning work; it just needs to know how to call the services. This separation of concerns is a hallmark of good robotics software design."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>c});var i=o(6540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);