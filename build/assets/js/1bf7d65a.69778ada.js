"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[8763],{2408:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"specifyplus/modules/vla-systems","title":"Module 4: Vision-Language-Action (VLA) Systems","description":"This module describes the highest level of the SpecifyPlus architecture, where Vision, Language, and Action are combined to create intelligent, autonomous behaviors. This layer enables natural and intuitive human-robot interaction.","source":"@site/docs/specifyplus/modules/vla-systems.md","sourceDirName":"specifyplus/modules","slug":"/specifyplus/modules/vla-systems","permalink":"/ai-humanoid-robotics-book/docs/specifyplus/modules/vla-systems","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var o=t(4848),i=t(8453);const r={},a="Module 4: Vision-Language-Action (VLA) Systems",c={},l=[{value:"System Architecture",id:"system-architecture",level:2},{value:"OpenAI Whisper for Voice",id:"openai-whisper-for-voice",level:2},{value:"LLM Planning",id:"llm-planning",level:2},{value:"Autonomous Humanoid Capstone",id:"autonomous-humanoid-capstone",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla-systems",children:"Module 4: Vision-Language-Action (VLA) Systems"})}),"\n",(0,o.jsx)(n.p,{children:"This module describes the highest level of the SpecifyPlus architecture, where Vision, Language, and Action are combined to create intelligent, autonomous behaviors. This layer enables natural and intuitive human-robot interaction."}),"\n",(0,o.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The VLA system acts as the primary interface between a human operator and the robot's low-level control and perception systems."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:'graph LR\n    A[Human Voice Command] --\x3e B{OpenAI Whisper};\n    B --\x3e C{LLM Planner};\n    C --\x3e D[Action Sequencer];\n    subgraph "Robot\'s AI Brain (Isaac)"\n        E[Perception System]\n    end\n    E --\x3e C;\n    D --\x3e F[ROS 2 Control Interfaces];\n'})}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-for-voice",children:"OpenAI Whisper for Voice"}),"\n",(0,o.jsx)(n.p,{children:"To enable voice-based commands, SpecifyPlus integrates OpenAI's Whisper model."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Function:"})," Whisper is a state-of-the-art automatic speech recognition (ASR) system. It takes an audio stream from the robot's microphone as input and transcribes it into text."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Node:"})," A dedicated ROS 2 node runs the Whisper model. It subscribes to an audio topic (e.g., ",(0,o.jsx)(n.code,{children:"/audio/mic_in"}),") and publishes the transcribed text to a string topic (e.g., ",(0,o.jsx)(n.code,{children:"/voice_command_text"}),")."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example Usage:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'User says: "Robot, please get me the water bottle from the table."'}),"\n",(0,o.jsx)(n.li,{children:"The microphone captures the audio."}),"\n",(0,o.jsxs)(n.li,{children:["The Whisper node transcribes the audio and publishes the string ",(0,o.jsx)(n.code,{children:'"Robot, please get me the water bottle from the table."'})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"llm-planning",children:"LLM Planning"}),"\n",(0,o.jsx)(n.p,{children:"The text command is then processed by a Large Language Model (LLM) which acts as a high-level task planner."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input:"})," The LLM receives the transcribed text command. It can also be provided with additional context from the robot's perception system, such as a list of currently visible objects and their locations."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Function:"})," The LLM's task is to decompose the high-level, ambiguous human command into a structured plan of concrete, executable actions for the robot. This involves reasoning about the user's intent and the current state of the world."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output:"})," The LLM outputs a sequence of actions in a structured format, like JSON."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example LLM Interaction:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input Command:"}),' "Get me the water bottle from the table."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception Context:"})," ",(0,o.jsx)(n.code,{children:'{ "visible_objects": ["water_bottle", "table", "chair"] }'})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Output Plan:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'[\n  { "action": "navigate_to", "target": "table" },\n  { "action": "detect_object", "object_name": "water_bottle" },\n  { "action": "grasp_object", "object_name": "water_bottle" },\n  { "action": "navigate_to", "target": "user" },\n  { "action": "release_object" }\n]\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"autonomous-humanoid-capstone",children:"Autonomous Humanoid Capstone"}),"\n",(0,o.jsx)(n.p,{children:'The final step is to translate the LLM\'s plan into physical actions. This is handled by an "Action Sequencer" or "Behavior Tree" node.'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Function:"})," This node subscribes to the plan from the LLM. It iterates through the sequence of actions and makes the appropriate ROS 2 service calls or topic publications to execute each step."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration:"})," It connects the abstract plan to the underlying robotics modules:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"navigate_to"}),": Makes a call to the Nav2 action server."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"detect_object"}),": Uses the perception system (e.g., Isaac ROS) to find the object's precise coordinates."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"grasp_object"}),": Calls a service on the manipulation node, which controls the robot's arm and gripper."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback:"})," The action sequencer monitors the success or failure of each action and can report the status back to the LLM, potentially allowing for re-planning if an error occurs."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This capstone integration brings together all the previous modules into a cohesive system that can understand and respond to high-level human commands in the real world."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);