"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[6342],{8181:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/prerequisites","label":"Prerequisites","docId":"prerequisites","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/learning-path","label":"Learning Path","docId":"learning-path","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/hardware-guide","label":"Hardware Guide","docId":"hardware-guide","unlisted":false},{"type":"category","label":"Module 1: ROS 2","items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-1-ros2/overview","label":"Module 1: The Robotic Nervous System (ROS 2)","docId":"module-1-ros2/overview","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-1-ros2/ros2-architecture","label":"ROS 2 Architecture","docId":"module-1-ros2/ros2-architecture","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-1-ros2/nodes-topics-services","label":"Nodes, Topics, and Services","docId":"module-1-ros2/nodes-topics-services","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-1-ros2/rclpy-python-agents","label":"rclpy - Python Agents for ROS 2","docId":"module-1-ros2/rclpy-python-agents","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-1-ros2/urdf-humanoids","label":"URDF for Humanoids","docId":"module-1-ros2/urdf-humanoids","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-1-ros2/practical-1-ros2-setup","label":"Practical 1: ROS 2 Setup","docId":"module-1-ros2/practical-1-ros2-setup","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Simulation","items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-2-simulation/overview","label":"Module 2: The Digital Twin (Gazebo & Unity)","docId":"module-2-simulation/overview","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-2-simulation/gazebo-physics","label":"Gazebo Physics","docId":"module-2-simulation/gazebo-physics","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-2-simulation/unity-rendering","label":"Unity Rendering","docId":"module-2-simulation/unity-rendering","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-2-simulation/sensor-simulation","label":"Sensor Simulation","docId":"module-2-simulation/sensor-simulation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-2-simulation/environment-building","label":"Environment Building","docId":"module-2-simulation/environment-building","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-2-simulation/practical-2-digital-twin","label":"Practical 2: Digital Twin","docId":"module-2-simulation/practical-2-digital-twin","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: NVIDIA Isaac","items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-3-isaac/overview","label":"Overview","docId":"module-3-isaac/overview","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-3-isaac/isaac-sim","label":"Isaac Sim","docId":"module-3-isaac/isaac-sim","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-3-isaac/isaac-ros","label":"Isaac ROS","docId":"module-3-isaac/isaac-ros","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-3-isaac/vslam-navigation","label":"vSLAM Navigation","docId":"module-3-isaac/vslam-navigation","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-3-isaac/nav2-bipedal","label":"Nav2 for Bipedal Robots","docId":"module-3-isaac/nav2-bipedal","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-3-isaac/practical-3-perception","label":"Practical 3: Perception","docId":"module-3-isaac/practical-3-perception","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: VLA","items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-4-vla/overview","label":"Overview","docId":"module-4-vla/overview","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-4-vla/whisper-voice","label":"Whisper Voice Control","docId":"module-4-vla/whisper-voice","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-4-vla/llm-planning","label":"LLM Planning","docId":"module-4-vla/llm-planning","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-4-vla/multimodal-integration","label":"Multimodal Integration","docId":"module-4-vla/multimodal-integration","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-4-vla/capstone-project","label":"Capstone: Autonomous Humanoid","docId":"module-4-vla/capstone-project","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/module-4-vla/practical-4-autonomous-humanoid","label":"Practical: Autonomous Humanoid","docId":"module-4-vla/practical-4-autonomous-humanoid","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Resources","items":[{"type":"link","href":"/ai-humanoid-robotics-book/docs/resources/hardware-setup","label":"Hardware Setup","docId":"resources/hardware-setup","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/resources/cloud-lab","label":"Cloud Lab","docId":"resources/cloud-lab","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/resources/assessments","label":"Assessments","docId":"resources/assessments","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/resources/references","label":"References","docId":"resources/references","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/resources/faq","label":"FAQ","docId":"resources/faq","unlisted":false},{"type":"link","href":"/ai-humanoid-robotics-book/docs/resources/glossary","label":"Glossary","docId":"resources/glossary","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"hardware-guide":{"id":"hardware-guide","title":"Hardware Guide","description":"This guide provides detailed specifications and recommendations for the hardware used in this course. While we provide a cloud alternative, having access to physical hardware will provide the best learning experience.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Bridging the gap between digital brain and physical body.","sidebar":"tutorialSidebar"},"learning-path":{"id":"learning-path","title":"Learning Path","description":"This course is structured as a 13-week program. Each week builds on the previous one, so it is recommended to follow the schedule in order.","sidebar":"tutorialSidebar"},"module-1-ros2/nodes-topics-services":{"id":"module-1-ros2/nodes-topics-services","title":"Nodes, Topics, and Services","description":"Now that we have a high-level overview of the ROS 2 architecture, let\'s dive deeper into the core components that make it work: nodes, topics, services, and actions.","sidebar":"tutorialSidebar"},"module-1-ros2/overview":{"id":"module-1-ros2/overview","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Welcome to Module 1. In this section, we will explore the Robot Operating System (ROS 2), the middleware that will serve as the nervous system for our humanoid robots.","sidebar":"tutorialSidebar"},"module-1-ros2/practical-1-ros2-setup":{"id":"module-1-ros2/practical-1-ros2-setup","title":"Practical 1: ROS 2 Setup","description":"In this practical, we will set up a ROS 2 workspace, create a package, and build the talker and listener nodes we wrote in the rclpy section.","sidebar":"tutorialSidebar"},"module-1-ros2/rclpy-python-agents":{"id":"module-1-ros2/rclpy-python-agents","title":"rclpy - Python Agents for ROS 2","description":"rclpy is the official Python client library for ROS 2. It provides a high-level, Pythonic interface to the ROS 2 ecosystem, allowing you to write your own ROS 2 nodes, publishers, subscribers, and more.","sidebar":"tutorialSidebar"},"module-1-ros2/ros2-architecture":{"id":"module-1-ros2/ros2-architecture","title":"ROS 2 Architecture","description":"The architecture of ROS 2 is designed to be a flexible, scalable, and robust platform for robotics. It is a significant evolution from ROS 1, built to support a wider range of applications, from small, single-board computers to large, multi-robot systems.","sidebar":"tutorialSidebar"},"module-1-ros2/urdf-humanoids":{"id":"module-1-ros2/urdf-humanoids","title":"URDF for Humanoids","description":"The Unified Robot Description Format (URDF) is an XML format for representing a robot model. In ROS 2, URDF is the standard way to describe the physical structure of a robot, including its links, joints, sensors, and visual appearance.","sidebar":"tutorialSidebar"},"module-2-simulation/environment-building":{"id":"module-2-simulation/environment-building","title":"Environment Building","description":"The environment is just as important as the robot itself in a simulation. A well-crafted environment allows for testing a wide range of scenarios and ensures that the simulation is a meaningful representation of the real world.","sidebar":"tutorialSidebar"},"module-2-simulation/gazebo-physics":{"id":"module-2-simulation/gazebo-physics","title":"Gazebo Physics","description":"Gazebo is a powerful, open-source 3D robotics simulator. At its core is a robust physics engine that allows you to simulate the dynamics of robots and their interactions with the environment with a high degree of realism.","sidebar":"tutorialSidebar"},"module-2-simulation/overview":{"id":"module-2-simulation/overview","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2. In this section, we will explore the concept of the digital twin and learn how to use powerful simulation tools like Gazebo and Unity to create realistic virtual environments for our robots.","sidebar":"tutorialSidebar"},"module-2-simulation/practical-2-digital-twin":{"id":"module-2-simulation/practical-2-digital-twin","title":"Practical 2: Digital Twin","description":"In this practical exercise, we\'ll bring together the concepts from this module to create a digital twin of a simple robot in a simulated environment. A digital twin is a virtual model of a physical object, used to simulate its behavior and monitor its state.","sidebar":"tutorialSidebar"},"module-2-simulation/sensor-simulation":{"id":"module-2-simulation/sensor-simulation","title":"Sensor Simulation","description":"A robot is only as good as its perception of the world, which is why accurate sensor simulation is critical for developing and testing robotics software. Simulators like Gazebo and Unity provide models for a wide range of common sensors.","sidebar":"tutorialSidebar"},"module-2-simulation/unity-rendering":{"id":"module-2-simulation/unity-rendering","title":"Unity Rendering","description":"While Gazebo is prized for its physics simulation, Unity stands out for its cutting-edge graphics and rendering capabilities. This makes it an excellent choice for tasks where visual realism is paramount, such as generating synthetic data for training computer vision models or creating compelling user interfaces for robot control.","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-ros":{"id":"module-3-isaac/isaac-ros","title":"Isaac ROS","description":"Isaac ROS is a collection of ROS 2 packages that are optimized to run on NVIDIA\'s Jetson and GPU platforms. These packages provide hardware acceleration for common robotics tasks, enabling real-time performance that would be difficult to achieve with a CPU alone.","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-sim":{"id":"module-3-isaac/isaac-sim","title":"Isaac Sim","description":"Isaac Sim is NVIDIA\'s robotics simulation platform, built on top of their Omniverse 3D collaboration and simulation framework. It offers a level of realism and performance that is a significant step up from traditional simulators like Gazebo.","sidebar":"tutorialSidebar"},"module-3-isaac/nav2-bipedal":{"id":"module-3-isaac/nav2-bipedal","title":"Nav2 for Bipedal Robots","description":"The ROS 2 Navigation Stack (Nav2) is a powerful and flexible framework for autonomous navigation. While it was originally designed for wheeled robots, it can be adapted to work with more complex platforms like bipedal robots (humanoids).","sidebar":"tutorialSidebar"},"module-3-isaac/overview":{"id":"module-3-isaac/overview","title":"Overview","description":"Welcome to Module 3! In this section, we\'ll dive into the NVIDIA Isaac ecosystem, a powerful platform for developing and deploying AI-powered robots. Isaac provides a suite of tools that accelerate the entire robotics workflow, from simulation to deployment.","sidebar":"tutorialSidebar"},"module-3-isaac/practical-3-perception":{"id":"module-3-isaac/practical-3-perception","title":"Practical 3: Perception","description":"In this practical exercise, you\'ll get hands-on experience with the Isaac ROS perception packages. You\'ll set up a simple perception pipeline in Isaac Sim and visualize the results in RViz.","sidebar":"tutorialSidebar"},"module-3-isaac/vslam-navigation":{"id":"module-3-isaac/vslam-navigation","title":"vSLAM Navigation","description":"Visual SLAM (vSLAM) is a technique for Simultaneous Localization and Mapping that uses camera data as its primary input. The Isaac ROS vSLAM package is a powerful, GPU-accelerated implementation that can provide robust and accurate real-time localization and mapping for robots.","sidebar":"tutorialSidebar"},"module-4-vla/capstone-project":{"id":"module-4-vla/capstone-project","title":"Capstone Project: The Autonomous Humanoid","description":"Final project integrating all modules: Voice-controlled humanoid with vision, planning, and autonomous navigation","sidebar":"tutorialSidebar"},"module-4-vla/llm-planning":{"id":"module-4-vla/llm-planning","title":"LLM Planning","description":"Large Language Models (LLMs) like GPT-4 have demonstrated remarkable abilities in reasoning and planning. This has opened up exciting new possibilities for using LLMs as the \\"brain\\" of a robot, responsible for high-level planning and decision-making.","sidebar":"tutorialSidebar"},"module-4-vla/multimodal-integration":{"id":"module-4-vla/multimodal-integration","title":"Multimodal Integration","description":"The real power of Vision-Language-Action (VLA) models comes from their ability to process and understand information from multiple modalities simultaneously. This is known as multimodal integration.","sidebar":"tutorialSidebar"},"module-4-vla/overview":{"id":"module-4-vla/overview","title":"Overview","description":"Welcome to Module 4! This is where we bring everything together to explore the exciting frontier of Vision-Language-Action (VLA) models in robotics. VLAs are AI models that can understand and respond to natural language commands, perceive the world through vision, and take actions to accomplish tasks.","sidebar":"tutorialSidebar"},"module-4-vla/practical-4-autonomous-humanoid":{"id":"module-4-vla/practical-4-autonomous-humanoid","title":"Practical 4: Building Your Autonomous Humanoid","description":"Step-by-step implementation guide for the capstone autonomous humanoid robot","sidebar":"tutorialSidebar"},"module-4-vla/whisper-voice":{"id":"module-4-vla/whisper-voice","title":"Whisper Voice Control","description":"Whisper is a state-of-the-art automatic speech recognition (ASR) system developed by OpenAI. It\'s incredibly versatile and can transcribe spoken language from a wide range of audio inputs, making it an excellent choice for adding voice control to a robot.","sidebar":"tutorialSidebar"},"prerequisites":{"id":"prerequisites","title":"Prerequisites","description":"To successfully complete this course, you will need a combination of hardware and software. We have designed the curriculum to be as accessible as possible, with a cloud-based lab alternative for those who do not have access to the recommended hardware.","sidebar":"tutorialSidebar"},"resources/assessments":{"id":"resources/assessments","title":"Assessments & Grading","description":"Complete assessment structure, rubrics, and evaluation criteria for Physical AI course","sidebar":"tutorialSidebar"},"resources/cloud-lab":{"id":"resources/cloud-lab","title":"Cloud Lab Setup: The Virtual Robotics Lab","description":"Complete guide to setting up your cloud-based robotics development environment","sidebar":"tutorialSidebar"},"resources/faq":{"id":"resources/faq","title":"Frequently Asked Questions (FAQ)","description":"Common questions and answers about Physical AI & Humanoid Robotics course","sidebar":"tutorialSidebar"},"resources/glossary":{"id":"resources/glossary","title":"Glossary of Terms","description":"Comprehensive glossary of terms used in Physical AI and Humanoid Robotics","sidebar":"tutorialSidebar"},"resources/hardware-setup":{"id":"resources/hardware-setup","title":"Hardware Setup Guide: Building Your Physical AI Lab","description":"Complete hardware guide for Physical AI & Humanoid Robotics - From budget to premium setups","sidebar":"tutorialSidebar"},"resources/references":{"id":"resources/references","title":"References & Further Reading","description":"Comprehensive bibliography of research papers, books, and resources for Physical AI and Humanoid Robotics","sidebar":"tutorialSidebar"},"specifyplus/api/authentication":{"id":"specifyplus/api/authentication","title":"API Authentication","description":"In the SpecifyPlus framework, \\"authentication\\" and \\"authorization\\" are handled by the ROS 2 Security features, also known as SROS2. This system provides a comprehensive security model for the robotics application, ensuring that only trusted nodes can communicate on the network."},"specifyplus/api/endpoints":{"id":"specifyplus/api/endpoints","title":"API Endpoints","description":"While SpecifyPlus does not have a traditional REST or GraphQL API, its functionality is exposed through the ROS 2 graph. This document maps ROS 2 concepts to familiar API terminology. ROS 2 Topics, Services, and Actions serve as the \\"endpoints\\" of the system."},"specifyplus/api/rate-limiting":{"id":"specifyplus/api/rate-limiting","title":"API Rate Limiting","description":"In the context of the SpecifyPlus system and ROS 2, \\"rate limiting\\" and traffic shaping are managed through two primary mechanisms: the publishing rate of nodes and the Quality of Service (QoS) policies."},"specifyplus/architecture":{"id":"specifyplus/architecture","title":"SpecifyPlus System Architecture","description":"This document provides a detailed look at the architecture of the SpecifyPlus system, breaking down the components and their interactions."},"specifyplus/deployment/cloud-deployment":{"id":"specifyplus/deployment/cloud-deployment","title":"Deployment: Cloud Deployment","description":"Deploying a full robotics stack like SpecifyPlus in the cloud presents unique challenges, particularly regarding GPU acceleration and real-time communication. This guide outlines strategies and best practices for cloud-based deployment."},"specifyplus/deployment/local-setup":{"id":"specifyplus/deployment/local-setup","title":"Deployment: Local Setup","description":"This guide details the steps required to set up the complete SpecifyPlus development environment on a local machine. The primary target is a Linux system (Ubuntu 22.04) with a compatible NVIDIA GPU."},"specifyplus/deployment/monitoring":{"id":"specifyplus/deployment/monitoring","title":"Deployment: Monitoring","description":"Monitoring the health and performance of a complex, distributed system like SpecifyPlus is crucial for debugging, performance tuning, and ensuring reliability. The ROS 2 ecosystem provides a rich set of tools for introspection and monitoring."},"specifyplus/getting-started":{"id":"specifyplus/getting-started","title":"Getting Started with SpecifyPlus","description":"This guide will walk you through setting up a minimal SpecifyPlus environment and running a basic simulation."},"specifyplus/integration/customization":{"id":"specifyplus/integration/customization","title":"Integration: Customization","description":"The SpecifyPlus framework is designed to be modular and extensible. This guide covers the most common ways to customize the system for your specific robot or application."},"specifyplus/integration/docusaurus":{"id":"specifyplus/integration/docusaurus","title":"Integration: Docusaurus","description":"Integrating outputs from the SpecifyPlus robotics system into a Docusaurus website can greatly enhance documentation by providing dynamic, visual content. This guide covers strategies for embedding robotics data and visualizations."},"specifyplus/integration/existing-projects":{"id":"specifyplus/integration/existing-projects","title":"Integration: Existing Projects","description":"Integrating an existing robotics project with the SpecifyPlus framework primarily involves ensuring compatibility with the ROS 2 communication layer. This guide provides steps for adapting your existing ROS 1 or ROS 2 projects."},"specifyplus/modules/ai-frameworks":{"id":"specifyplus/modules/ai-frameworks","title":"Module 3: AI Frameworks (NVIDIA Isaac)","description":"This module details the integration of NVIDIA\'s Isaac SDK for accelerating AI and perception tasks within the SpecifyPlus system. The Isaac ecosystem provides powerful tools for building the \\"brain\\" of the robot."},"specifyplus/modules/ros2-integration":{"id":"specifyplus/modules/ros2-integration","title":"Module 1: ROS 2 Integration","description":"This module documentation covers the integration of the Robotic Nervous System (ROS 2) within the SpecifyPlus framework. ROS 2 forms the communication backbone of the entire system."},"specifyplus/modules/simulation-tools":{"id":"specifyplus/modules/simulation-tools","title":"Module 2: Simulation Tools","description":"This module covers the use of simulation tools to create a \\"Digital Twin\\" of the robot and its environment. Simulation is a critical part of the SpecifyPlus workflow, enabling rapid development, testing, and data generation."},"specifyplus/modules/vla-systems":{"id":"specifyplus/modules/vla-systems","title":"Module 4: Vision-Language-Action (VLA) Systems","description":"This module describes the highest level of the SpecifyPlus architecture, where Vision, Language, and Action are combined to create intelligent, autonomous behaviors. This layer enables natural and intuitive human-robot interaction."},"specifyplus/overview":{"id":"specifyplus/overview","title":"SpecifyPlus System Overview","description":"Purpose"}}}}')}}]);