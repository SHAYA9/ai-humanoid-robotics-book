"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[1646],{7573:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"labs-and-exercises/vla-capstone-project/cognitive-planning","title":"8.3 Cognitive Planning: Natural Language to Actions","description":"The most innovative part of our VLA architecture is the use of a Large Language Model (LLM) as a high-level task planner. The LLM\'s vast knowledge of language and reasoning allows it to bridge the ambiguous, flexible world of human commands with the structured, precise world of robot operations.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/03-cognitive-planning.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/cognitive-planning","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/03-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"8.2 The Whisper \u2192 LLM \u2192 ROS 2 Pipeline","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/pipeline"},"next":{"title":"8.4 Object Detection and Manipulation","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/object-detection-and-manipulation"}}');var s=o(4848),i=o(8453);const r={sidebar_position:3},a="8.3 Cognitive Planning: Natural Language to Actions",l={},c=[{value:"The Power of Prompt Engineering and Tool Use",id:"the-power-of-prompt-engineering-and-tool-use",level:3},{value:"Example Interaction",id:"example-interaction",level:3},{value:"The Executor",id:"the-executor",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"83-cognitive-planning-natural-language-to-actions",children:"8.3 Cognitive Planning: Natural Language to Actions"})}),"\n",(0,s.jsx)(n.p,{children:"The most innovative part of our VLA architecture is the use of a Large Language Model (LLM) as a high-level task planner. The LLM's vast knowledge of language and reasoning allows it to bridge the ambiguous, flexible world of human commands with the structured, precise world of robot operations."}),"\n",(0,s.jsx)(n.h3,{id:"the-power-of-prompt-engineering-and-tool-use",children:"The Power of Prompt Engineering and Tool Use"}),"\n",(0,s.jsxs)(n.p,{children:["We don't train the LLM; we ",(0,s.jsx)(n.strong,{children:"prompt"})," it. We will use a technique often called ",(0,s.jsx)(n.strong,{children:"Tool Use"})," or ",(0,s.jsx)(n.strong,{children:"Function Calling"}),'. We give the LLM a special "system prompt" that describes the robot\'s capabilities as a set of tools (or functions) it can use.']}),"\n",(0,s.jsx)(n.p,{children:"The system prompt will define:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The LLM's Role:"}),' "You are a helpful robotics assistant. Your job is to convert user commands into a structured plan using the provided functions."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Available Functions:"})," We will describe each low-level robot skill, its purpose, and its required parameters.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"find_object(object_name: str)"}),": Locates an object and returns its coordinates."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"move_to(x: float, y: float, z: float)"}),": Moves the arm to a specific coordinate."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"set_gripper(state: str)"}),": Opens or closes the gripper ('open' or 'close')."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Desired Output Format:"})," We will instruct the LLM to always respond with a JSON object representing a sequence of these function calls."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-interaction",children:"Example Interaction"}),"\n",(0,s.jsxs)(n.p,{children:["Let's trace a command through the ",(0,s.jsx)(n.code,{children:"CognitivePlannerNode"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:["1. User Command (from ",(0,s.jsx)(n.code,{children:"/voice_command"})," topic):"]}),"\n",(0,s.jsx)(n.code,{children:'"Please pick up the red cube and put it on the blue cylinder."'})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. The Node Constructs the Full Prompt to the LLM:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"System Prompt:\nYou are a robotics assistant... You have access to the following tools:\n- find_object(object_name: str)\n- move_to(x: float, y: float, z: float)\n- set_gripper(state: str ['open'|'close'])\nPlease respond with a JSON list of tool calls.\n\nUser Prompt:\n\"Please pick up the red cube and put it on the blue cylinder.\"\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"3. The LLM Processes the Prompt and Returns a Structured Plan:"}),"\nThe LLM uses its reasoning capabilities to decompose the command into a logical sequence of steps using ",(0,s.jsx)(n.em,{children:"only the tools it was given"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LLM Response (JSON):"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'[\n  {"tool": "find_object", "params": {"object_name": "red cube"}},\n  {"tool": "move_to", "params": {"x": 0.5, "y": 0.2, "z": 0.1}}, // Placeholder coords from find\n  {"tool": "set_gripper", "params": {"state": "close"}},\n  {"tool": "find_object", "params": {"object_name": "blue cylinder"}},\n  {"tool": "move_to", "params": {"x": -0.3, "y": 0.4, "z": 0.2}}, // Placeholder coords from find\n  {"tool": "set_gripper", "params": {"state": "open"}}\n]\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:["(Note: In a real implementation, the ",(0,s.jsx)(n.code,{children:"move_to"})," calls would be dynamically filled in by the ",(0,s.jsx)(n.code,{children:"find_object"})," results.)"]})}),"\n",(0,s.jsx)(n.h3,{id:"the-executor",children:"The Executor"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"CognitivePlannerNode"})," receives this JSON. Its final job is to be an ",(0,s.jsx)(n.strong,{children:"executor"}),". It parses the JSON array and iterates through it, making the actual ROS 2 service calls for each step in the sequence. It calls the vision service to get coordinates, then the motion planning service to move the arm, and so on."]}),"\n",(0,s.jsxs)(n.p,{children:["This architecture is incredibly powerful because if you want to add a new skill to your robot (e.g., ",(0,s.jsx)(n.code,{children:"rotate_wrist"}),"), you don't need to retrain a complex AI model. You simply add the new tool to the LLM's system prompt and implement the corresponding ROS 2 service. The LLM can then intelligently incorporate this new skill into its plans."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);