"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[9299],{1737:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"labs-and-exercises/vla-capstone-project/pipeline","title":"8.2 The Whisper \u2192 LLM \u2192 ROS 2 Pipeline","description":"The core of our VLA system is the data flow that converts human speech into a series of executable robot commands. This pipeline consists of three main stages, each handled by a dedicated ROS 2 node.","source":"@site/docs/05-labs-and-exercises/01-vla-capstone-project/02-pipeline.md","sourceDirName":"05-labs-and-exercises/01-vla-capstone-project","slug":"/labs-and-exercises/vla-capstone-project/pipeline","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/01-vla-capstone-project/02-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"8.1 The VLA Capstone Project","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/introduction"},"next":{"title":"8.3 Cognitive Planning: Natural Language to Actions","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/vla-capstone-project/cognitive-planning"}}');var t=s(4848),o=s(8453);const r={sidebar_position:2},a="8.2 The Whisper \u2192 LLM \u2192 ROS 2 Pipeline",c={},l=[{value:"1. Speech-to-Text (Whisper)",id:"1-speech-to-text-whisper",level:3},{value:"2. Text-to-Plan (Large Language Model)",id:"2-text-to-plan-large-language-model",level:3},{value:"3. Plan-to-Action (ROS 2)",id:"3-plan-to-action-ros-2",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"82-the-whisper--llm--ros-2-pipeline",children:"8.2 The Whisper \u2192 LLM \u2192 ROS 2 Pipeline"})}),"\n",(0,t.jsx)(n.p,{children:"The core of our VLA system is the data flow that converts human speech into a series of executable robot commands. This pipeline consists of three main stages, each handled by a dedicated ROS 2 node."}),"\n",(0,t.jsx)(n.h3,{id:"1-speech-to-text-whisper",children:"1. Speech-to-Text (Whisper)"}),"\n",(0,t.jsxs)(n.p,{children:["The first step is to convert spoken language into text. We will use ",(0,t.jsx)(n.strong,{children:"OpenAI's Whisper"}),", a state-of-the-art speech recognition model."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node:"})," A ",(0,t.jsx)(n.code,{children:"WhisperNode"})," will be responsible for this stage."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input:"})," It will listen to an audio stream from a microphone (or a pre-recorded audio file for testing)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Processing:"})," It uses a local or API-based version of Whisper to transcribe the audio."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output:"})," The transcribed text is published as a ",(0,t.jsx)(n.code,{children:"std_msgs/msg/String"})," to a ROS 2 topic named ",(0,t.jsx)(n.code,{children:"/voice_command"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-text-to-plan-large-language-model",children:"2. Text-to-Plan (Large Language Model)"}),"\n",(0,t.jsx)(n.p,{children:'This is the "cognitive" core of our system. This stage takes the transcribed text and converts it into a structured plan that the robot can understand.'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node:"})," A ",(0,t.jsx)(n.code,{children:"CognitivePlannerNode"})," subscribes to the ",(0,t.jsx)(n.code,{children:"/voice_command"})," topic."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Processing:"})," When it receives a text command, it queries a ",(0,t.jsx)(n.strong,{children:"Large Language Model (LLM)"}),", such as GPT-4 or a local model like Llama 3. The magic happens in the ",(0,t.jsx)(n.strong,{children:"prompt"}),': we will instruct the LLM to act as a robotics planner and convert the user\'s command into a JSON array of specific, pre-defined robot "skills" or functions.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Text:"}),' "put the apple on the plate"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM Output (JSON):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'[\n  {"skill": "find", "object": "apple"},\n  {"skill": "pick", "object": "apple"},\n  {"skill": "find", "object": "plate"},\n  {"skill": "place", "target": "plate"}\n]\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-plan-to-action-ros-2",children:"3. Plan-to-Action (ROS 2)"}),"\n",(0,t.jsxs)(n.p,{children:["Once the ",(0,t.jsx)(n.code,{children:"CognitivePlannerNode"})," has the structured JSON plan from the LLM, it acts as an executor, translating each step of the plan into ROS 2 commands."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Loop:"})," The node iterates through the array of skills."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dispatching:"})," For each skill, it makes the appropriate ROS 2 service call or action request to the nodes responsible for the robot's low-level capabilities (vision, motion, grasping).","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'{"skill": "find", ...}'})," \u2192 Call the ",(0,t.jsx)(n.code,{children:"/vision/find_object"})," service."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'{"skill": "pick", ...}'})," \u2192 Call the ",(0,t.jsx)(n.code,{children:"/motion/move_to"})," and ",(0,t.jsx)(n.code,{children:"/gripper/close"})," services."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:(0,t.jsx)("p",{align:"center",children:"PLACEHOLDER: VLA Data Flow Diagram"})}),"\n",(0,t.jsx)(n.em,{children:(0,t.jsxs)("p",{align:"center",children:["A diagram showing the three stages: A microphone icon inputs to the Whisper Node, which publishes a String to the ",(0,t.jsx)(n.code,{children:"/voice_command"})," topic. The Cognitive Planner Node subscribes to this, communicates with an LLM (cloud icon), and then makes ROS 2 service/action calls to the Vision, Motion, and Gripper nodes."]})})]}),"\n",(0,t.jsx)(n.p,{children:"This pipeline effectively decouples the complexity of language understanding from the robot's core functionalities. The LLM acts as a high-level \"brain,\" translating human intent into a structured format that the robot's more deterministic systems can execute."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);