"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[3258],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var r=i(6540);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}},9702:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"labs-and-exercises/lab-03-reinforcement-learning-reach-task","title":"Lab 3: Reinforcement Learning - Reach Task","description":"Goal: Train a Franka robot to reach a target using reinforcement learning.","source":"@site/docs/05-labs-and-exercises/04-lab-03-reinforcement-learning-reach-task.md","sourceDirName":"05-labs-and-exercises","slug":"/labs-and-exercises/lab-03-reinforcement-learning-reach-task","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-03-reinforcement-learning-reach-task","draft":false,"unlisted":false,"editUrl":"https://github.com/SHAYA9/ai-humanoid-robotics-book/tree/main/docs/05-labs-and-exercises/04-lab-03-reinforcement-learning-reach-task.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lab 2: Controlling a Robot","permalink":"/ai-humanoid-robotics-book/docs/labs-and-exercises/lab-02-controlling-a-robot"}}');var t=i(4848),s=i(8453);const a={sidebar_position:4},o="Lab 3: Reinforcement Learning - Reach Task",l={},c=[{value:"Part 1: Setting up the Environment",id:"part-1-setting-up-the-environment",level:3},{value:"Part 2: Modifying the Environment for a Reach Task",id:"part-2-modifying-the-environment-for-a-reach-task",level:3},{value:"Part 3: Training the Agent",id:"part-3-training-the-agent",level:3}];function h(e){const n={code:"code",h1:"h1",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lab-3-reinforcement-learning---reach-task",children:"Lab 3: Reinforcement Learning - Reach Task"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal:"})," Train a Franka robot to reach a target using reinforcement learning."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Estimated Time:"})," 2 hours"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:["This lab is more involved than the previous ones. We will be using the ",(0,t.jsx)(n.code,{children:"omni.isaac.gym"})," extension to train our RL agent."]}),"\n",(0,t.jsx)(n.h3,{id:"part-1-setting-up-the-environment",children:"Part 1: Setting up the Environment"}),"\n",(0,t.jsxs)(n.p,{children:["Isaac Sim provides example environments for common RL tasks. We'll use the ",(0,t.jsx)(n.code,{children:"FrankaCabinet"})," environment as a starting point and modify it for our reach task."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Locate the Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The RL examples are located in the ",(0,t.jsx)(n.code,{children:"python/examples/reinforcement_learning"})," directory of your Isaac Sim installation."]}),"\n",(0,t.jsxs)(n.li,{children:["Find the ",(0,t.jsx)(n.code,{children:"franka_cabinet.py"})," file."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create a Copy:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Make a copy of ",(0,t.jsx)(n.code,{children:"franka_cabinet.py"})," and name it ",(0,t.jsx)(n.code,{children:"franka_reach.py"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run the Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["To run the training, you'll use the ",(0,t.jsx)(n.code,{children:"python.sh"})," script that comes with Isaac Sim."]}),"\n",(0,t.jsxs)(n.li,{children:["Open a terminal and run:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./python.sh python/examples/reinforcement_learning/franka_reach.py\n"})}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"This will start the training process. You should see a window with many Franka robots training in parallel."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"part-2-modifying-the-environment-for-a-reach-task",children:"Part 2: Modifying the Environment for a Reach Task"}),"\n",(0,t.jsxs)(n.p,{children:["Now, we'll modify ",(0,t.jsx)(n.code,{children:"franka_reach.py"})," to change the task from opening a cabinet to simply reaching a target."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simplify the Scene:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["In the ",(0,t.jsx)(n.code,{children:"FrankaCabinet"})," class, find the ",(0,t.jsx)(n.code,{children:"set_up_scene"})," method."]}),"\n",(0,t.jsx)(n.li,{children:"Remove the code that adds the cabinet to the scene. We only need the robot."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define the Observation Space:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["In the ",(0,t.jsx)(n.code,{children:"get_observations"}),' method, we need to define what the robot "sees". For a reach task, this should include:',"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The robot's joint positions."}),"\n",(0,t.jsx)(n.li,{children:'The position of the end-effector (the "hand").'}),"\n",(0,t.jsx)(n.li,{children:"The position of the target."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define the Reward Function:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"calculate_metrics"})," method is where you define the reward. A good reward function for a reach task would be:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A dense reward based on the negative distance between the end-effector and the target. The closer it gets, the higher the reward."}),"\n",(0,t.jsx)(n.li,{children:"A sparse reward for successfully reaching the target."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define the Goal:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"At the beginning of each episode, you'll need to randomize the position of the target."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"part-3-training-the-agent",children:"Part 3: Training the Agent"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run the Training Again:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["After modifying the script, run it again from the terminal:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./python.sh python/examples/reinforcement_learning/franka_reach.py\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Observe the Training:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Watch the ",(0,t.jsx)(n.code,{children:"mean reward"})," in the terminal output. It should steadily increase as the robot learns to reach the target."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test the Policy:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Once the training is complete, you can test the learned policy by running the script with the ",(0,t.jsx)(n.code,{children:"--test"})," flag:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./python.sh python/examples/reinforcement_learning/franka_reach.py --test\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Congratulations!"})," You've trained your first RL agent in Isaac Sim. This is a foundational skill for tackling more complex robotics tasks."]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);