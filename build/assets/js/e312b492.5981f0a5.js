"use strict";(globalThis.webpackChunkai_humanoid_robotics_book_new=globalThis.webpackChunkai_humanoid_robotics_book_new||[]).push([[7505],{1884:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/overview","title":"Overview","description":"Welcome to Module 4! This is where we bring everything together to explore the exciting frontier of Vision-Language-Action (VLA) models in robotics. VLAs are AI models that can understand and respond to natural language commands, perceive the world through vision, and take actions to accomplish tasks.","source":"@site/docs/module-4-vla/overview.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/overview","permalink":"/ai-humanoid-robotics-book/docs/module-4-vla/overview","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Overview"},"sidebar":"tutorialSidebar","previous":{"title":"Practical 3: Perception","permalink":"/ai-humanoid-robotics-book/docs/module-3-isaac/practical-3-perception"},"next":{"title":"Whisper Voice Control","permalink":"/ai-humanoid-robotics-book/docs/module-4-vla/whisper-voice"}}');var t=o(4848),a=o(8453);const r={sidebar_position:1,title:"Overview"},s="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"The VLA Paradigm",id:"the-vla-paradigm",level:2},{value:"Key Components",id:"key-components",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsxs)(n.p,{children:["Welcome to Module 4! This is where we bring everything together to explore the exciting frontier of ",(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," models in robotics. VLAs are AI models that can understand and respond to natural language commands, perceive the world through vision, and take actions to accomplish tasks."]}),"\n",(0,t.jsx)(n.h2,{id:"the-vla-paradigm",children:"The VLA Paradigm"}),"\n",(0,t.jsx)(n.p,{children:"The core idea behind VLAs is to create a single, end-to-end model that can map from multi-modal inputs (e.g., camera images, voice commands) to robot actions (e.g., motor commands). This is a departure from traditional robotics, where perception, planning, and control are often treated as separate, hand-engineered modules."}),"\n",(0,t.jsx)(n.h2,{id:"key-components",children:"Key Components"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision Encoder:"})," Processes the raw visual input from cameras and extracts meaningful features."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Encoder:"})," Processes the natural language command and encodes its meaning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Decoder:"})," Takes the combined vision and language features and generates a sequence of actions for the robot to execute."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the basic architecture of a Vision-Language-Action model."}),"\n",(0,t.jsx)(n.li,{children:"Appreciate the role of large language models (LLMs) in robotics."}),"\n",(0,t.jsx)(n.li,{children:"Learn how to integrate voice commands using a speech-to-text system like Whisper."}),"\n",(0,t.jsx)(n.li,{children:"Explore how to combine multi-modal inputs for robot control."}),"\n",(0,t.jsx)(n.li,{children:"Build a simple VLA-based capstone project."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);