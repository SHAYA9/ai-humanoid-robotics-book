---
sidebar_position: 3
---

# 7.3 Sensor Simulation

One of the most powerful features of Gazebo is its ability to simulate a wide range of robotic sensors. The data generated by these virtual sensors is published to ROS 2 topics, just like data from real hardware. This allows you to test your entire perception and control stack in the simulation without needing a physical robot.

### How Sensor Simulation Works

Gazebo simulates sensors using **plugins**. A plugin is a piece of code that attaches to a robot model and generates sensor data based on the state of the virtual world. For common sensors, you don't need to write code; you can simply add a `<sensor>` tag to your SDF file and configure its properties.

### Common Sensor Types

Hereâ€™s how Gazebo simulates the most common sensors in robotics:

#### LiDAR (Laser Range Finder)

-   **Concept:** Gazebo simulates LiDAR by performing GPU-accelerated **ray casting**. It shoots out thousands of virtual laser beams from the sensor's origin. When a beam hits a collision object in the world, Gazebo calculates the distance and returns it as a data point.
-   **Configuration:** You can specify the sensor's update rate, range (min/max distance), field of view, and resolution (number of beams). You can also add a noise model to simulate the imperfections of a real LiDAR.
-   **Output:** The plugin publishes `sensor_msgs/msg/LaserScan` messages to a ROS 2 topic.

#### IMU (Inertial Measurement Unit)

-   **Concept:** The IMU plugin accesses the physics engine's ground truth state for the link it's attached to. It reads the link's linear acceleration and angular velocity.
-   **Configuration:** The most important feature is the ability to add realistic noise. You can configure Gaussian noise and random drift (bias) to the accelerometer and gyroscope readings, which is crucial for testing state estimation and localization algorithms.
-   **Output:** The plugin publishes `sensor_msgs/msg/Imu` messages.

#### Depth Camera

-   **Concept:** A depth camera works by rendering the scene from its perspective, but instead of outputting color, it outputs the distance to each pixel. This creates a **depth map**.
-   **Configuration:** You can set the camera's resolution, field of view, and clipping planes (near and far distances). The plugin can also be configured to generate a 3D point cloud directly.
-   **Output:** The plugin can publish `sensor_msgs/msg/Image` (for the depth map) or `sensor_msgs/msg/PointCloud2` messages.

By adding these simulated sensors to your robot's SDF model, you can create a Digital Twin that provides a rich, realistic stream of data for testing your perception, navigation, and control software.
